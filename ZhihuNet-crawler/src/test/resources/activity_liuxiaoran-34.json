{
  "data": [
    {
      "id": "1752309907385",
      "type": "feed",
      "target": {
        "id": "1906936848632256357",
        "type": "article",
        "author": {
          "id": "f16a06041ffac91a1c0d31da4658f5a8",
          "name": "sonta",
          "headline": "PhD student @ MIT CSAIL",
          "type": "people",
          "user_type": "people",
          "url": "https://www.zhihu.com/people/buhezuobugaoxing",
          "url_token": "buhezuobugaoxing",
          "avatar_url": "https://picx.zhimg.com/v2-b1712309c6b65f0a7b292122b4dd82f9_l.jpg?source=5a24d060&needBackground=1",
          "gender": 0,
          "is_following": false,
          "is_followed": false,
          "is_org": false,
          "badge": [],
          "badge_v2": {
            "title": "",
            "merged_badges": [],
            "detail_badges": [],
            "icon": "",
            "night_icon": ""
          },
          "vip_info": {
            "is_vip": true,
            "vip_icon": {
              "url": "https://pic1.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a_r.jpg?source=5a24d060",
              "night_mode_url": "https://picx.zhimg.com/v2-c9686ff064ea3579730756ac6c289978_r.jpg?source=5a24d060"
            },
            "target_url": ""
          },
          "kvip_info": {
            "is_vip": false,
            "target_url": ""
          }
        },
        "created": 1751716480,
        "updated": 1751716480,
        "title": "PhD 第四学期 年中随笔",
        "excerpt_title": "",
        "content": "<blockquote data-pid=\"WDnQQi2G\">本来想着五月底写完的，然后写了一半就不幸开摆了。这次趁着独立日假日来一口气写完</blockquote><p data-pid=\"P54_e31t\">前期回顾传送门：</p><p data-pid=\"pREF9_6R\"><a href=\"https://zhuanlan.zhihu.com/p/674624059\" class=\"internal\" target=\"_blank\">sonta：PhD第一学期 年末随笔</a></p><p data-pid=\"AUEnUoFo\"><a href=\"https://zhuanlan.zhihu.com/p/702482919\" class=\"internal\" target=\"_blank\">sonta：PhD 第二学期随笔</a></p><p data-pid=\"Ur2MOjAm\"><a href=\"https://zhuanlan.zhihu.com/p/12745534263\" class=\"internal\" target=\"_blank\">sonta：PhD 第三学期年末随笔</a></p><p data-pid=\"n5tr1yGB\">不知不觉已经到了这个系列的第四集啦，这一集至少是整个系列的中场，因为自己并没有读满五年的打算。</p><p data-pid=\"CYjn2T9W\">既然是一个整体的中场时刻，那么在这篇中，会有很多总结以及回顾向的内容，用来阐述自己在Research哲学以及心路历程，和一些生活感悟，hopefully能给junior researcher以及ND（ADHD+ASD）的朋友们带来一些启发。</p><h2>Me and My Research</h2><p data-pid=\"sliLZh7u\">站在现在这个节点（2025年7月），自己在LLM新架构领域做的也算是小（大？）有名气，在这里作为一个中场回顾，阐述一下自己的research哲学，分享一下自己做工作的心路历程 （特地选了一个跟You and your research这篇经典article相映成趣的主标题）</p><h3>Balancing interest and impact</h3><p data-pid=\"UgPcdazc\">这个副标题可以说是每个Researcher都会思考并且犯难的问题。</p><p data-pid=\"9c2R8jQv\">ASD的特质就是兴趣范围特别的窄，那么作为一个（highly likely）ASD+INTP来说，自己做不来一点不感兴趣的东西也是正常不过了。所以我对副标题的答案自然是Interest first，兼顾Impact。既然自己是算法+GPU并行爱好者，并且新架构领域有一定的关注度，那么做这个方向也是再正常不过。</p><p data-pid=\"OkpAWrQy\">每个人的研究兴趣跟早年的经历非常相关，像自己早年对线性代数这门课情有独钟，对Gilbert Strang 老爷子的公开课和教材爱不释手（夸夸南科大会选教材）。之后又对CS里面的算法和数据结构和并行算法兴趣颇深。所以在做Parsing的时候的体现，每天想的并不是语言学，而是想的是如何在GPU上并行动态规划算法，成天用CUDA写parsing动态规划算法（CKY，Eisner，PCFG，PLCFRS，etc）。</p><p data-pid=\"UjMvvOY_\">自己最开始reachout老板做暑研老板回复I am a big fan of your work的时候实属是让自己受宠若惊。老板录我肯定不是因为爱看我做parsing，而是看重我做parsing的时候所展示出来的technical水平。Parsing research固然好玩，并且开发智力，但影响力为0。<sup data-text=\"有UW的童鞋就告诉我某教授说五年前可能会录我，但现在不行，因为parsing nobody cares\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"1\">[1]</sup> </p><p data-pid=\"zxrlA4sN\">最开始录我的时候老板就建议我换一个完全不同的方向去做一些<b>更有影响力</b>的工作，因为做parsing算法的人早就深知sonta之厉害，那么，应该去做点其他的方向，让大家知道，sonta不光只会做parsing，做任意一个领域都能做的很好。既然老板都这么说了，加上自己也无意吊死在一个没有任何前途的方向上，转行也是板上钉钉的事情。</p><p data-pid=\"vJf6h_xx\">转行的时候正值LLM开始横扫的年代，很庆幸自己申请季的时候刚好赶上了ChatGPT的发布，(bad) old NLP task-by-task的research被无情的扫进了历史的垃圾桶（好死），让转行的沉没成本为0。要不然读了两年PhD才开始全面换方向还是很难受的。申请季的半年窗口让自己有充足的时间有探索自己在PhD想做什么课题，对自己的兴趣和尿性很了解的sonta先排除了所有prompt相关的领域，对老板说打死我也不做prompting，只做modeling。在LLM时代，efficiency的重要度空前提高，自己偶然发现了HazyResearch这个宝藏group，并对HazyResearch的理念深以为然，仿佛打开了新世界的大门，一发不可收拾。序列建模+新架构简直是完美契合自己research interest，既强调硬件上的efficiency，又能做许多非常有趣的算法。另外一个有意思的现象是，之前做过Parsing的modeling爱好者很多都转到了序列建模这个领域，比方说Yikang, Xuezhe，以及被我拐来的 <a class=\"member_mention\" href=\"https://www.zhihu.com/people/8d5f4c3b705ef7df799b680515700468\" data-hash=\"8d5f4c3b705ef7df799b680515700468\" data-hovercard=\"p$b$8d5f4c3b705ef7df799b680515700468\">@yzhangcs</a> ，并且都做的很好，序列建模属于是Parsing老玩家下岗再就业的绝佳去处了。</p><p data-pid=\"Bloelt3f\">在方向的选择上，老板的taste跟指导也很有帮助，在22年6月就跟我强烈安利FlashAttention这个工作（当时FlashAttenton这个工作的arxiv刚出），并且他自己的group在朝着MLSys的方向不断evolve（MIT 0个纯NLP group）。老板在最开始有循循善诱我做一些硬件高效+数值线代相关的可以发挥自己特长的方向，虽然自己跟老板pitch线性模型的时候老板非常怀疑其长文本能力，但老板还是很相信我能做好所有我想做的东西，给予了充分的支持和自由度，才能开启了一段属于自己research journey的故事。</p><p data-pid=\"qXCK_uKL\">自己在推特上的简介一直写的是 Working on scalable and principled methods。Algorithm + Coding + 理论 爱好者 sonta 的 research philosophy 就是尽量在high-quality data is all you need” 的时代背景重新找回做modeling research的魅力（我就不信架构不重要）。做一些既能 scaling（algorithm+coding）、又能从原理上有清晰 justification （理论） 的 work，是真的很快乐呜呜呜。  做自己相信的东西真的好幸福呜呜，比起 master 年代日常 routine 中反复上演的存在主义危机，现在的我能把自己 position 到一个相信 algorithm 在 scaling 年代依然有价值的轨道上，真的能把 coding + algorithm 的兴趣发挥到极致。</p><a href=\"https://www.zhihu.com/question/658848345/answer/78028238211\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">PhD期间如何保持科研干劲避免burn out？</a><h3>Research jouney</h3><p data-pid=\"eR_8L4oR\">最开始做序列建模没多久正逢RetNet的发布。 在此基础上，结合自己对RNN遗忘门以及对硬件高效算法的信仰，做了Gated Linear Attention (GLA)。但是恰巧撞车了同期<i>现象级</i>的Mamba，在看到Mamba NeurIPS前一周arxiv的时候，逼老板<sup data-text=\"老板最开始思想还是有点问题，恨不得polish到完美之前都不arxiv，现在已老实，甚至会主动让我们早点arxiv\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"2\">[2]</sup>同意自己在neurips ‘23之前挂出arxiv，方便自己去宣传（和social）。第一次开conference的sonta可能借着新鲜感和兴奋劲，逢人便宣传GLA。虽然这个领域的肉基本被Mamba吃了，但好歹还是可以跟着喝了不少汤的。Mamba总体来说极大的激发了大家对这个领域的兴趣，对领域的发展是极好的事，毕竟一个领域良性发展最怕的就是没有关注度。而踩在领域火爆之前入场并积累，怎么说也算是一件幸事。</p><p data-pid=\"Nz1Cgbns\">24年初Zoology和Repeat After Me的这两篇工作把RNN的in-context recall的问题摆在了台面上。但这个问题并不是一个新问题，而是RNN数十年研究以来的一个老生常谈的问题。在新时代的背景下，这个问题重新受到了广泛关注，下一个工作便很自然的focus到了改善RNN的in-context recall问题上面。</p><p data-pid=\"U80rq2wo\">喜欢从第一性原理解决问题的sonta自然马上想到了DeltaNet这个从associative memory视角出发的经典老工作。其实intuition很简单，就是每个key应该只对应一个value，这样我们才能通过key来retrieve value。模型需要动态的决定需要保留输入的key-value association pair，还是已经存在associative memory （i.e., RNN state）的旧的key-value association pair。这个决定的过程是数据驱动的动态的过程。</p><p data-pid=\"DZRRn-Wi\">虽说这个算法相当直觉，但DeltaNet在那个时候大家并不知道怎么高效训练（其实vanilla线性注意力的chunk并行在那个时候才开始受到重视），那么已经玩了一个学期GLA并行训练之后自然而然的就是想拿着chunk并行这个锤子，看能不能敲开DeltaNet这颗好玩的钉子。</p><p data-pid=\"FgCIQcjp\">这个问题我甚至从23年9月的时候就在思考，但当时没有想明白（实在是nontrivial的一个问题）。在深入研究了GLA的并行之后回到这个问题，我马上意识到需要研究下这个RNN展开之后形式能不能高效计算。展开RNN的话，首先要研究这个RNN的transition matrix是什么，以及它累乘的性质。在我意识到DeltaNet的transition matrix是一个identity-plus-low-rank的类似Householder矩阵的时候，随手google了一下householder矩阵累乘有没有什么高效算法，然后就翻到了「WY表示」这个80年代的经典工作。点进去一看，发现了非常眼熟的外积累加的形式，这不正是linear attention里面的常见的形式吗？有外积累加就意味着可以写成矩阵乘法，也就意味着在硬件上很可能是高效的，嗅觉敏锐+算法直觉无敌好的sonta立刻意识到了这个算法跟chunk并行算法的兼容性，最后推出了一个自己非常满意的优雅算法，让DeltaNet从文献的垃圾堆里面翻出来重焕新生，感觉像是两波聪明绝顶的人跨越时空的灵魂交流，这种感觉还是相当令人有满足和成就感的（考古爱好者sonta之前做parsing的时候就很喜欢这么干）。很多人看完这个算法都被美到，比方说老板实在是惊讶我是从怎么想出来这么一个天才算法的，属于是sonta算法巅峰造极的一个代表作。</p><p data-pid=\"RwD265dd\">暑假实习的时候，因为谈好的是做Subquadratic架构，所以先闭着眼睛做了一些Mamba2和DeltaNet的比较，发现Mamba2虽然在MQAR上比不过DeltaNet，但在很多下游task下面薄纱DeltaNet。本着打不过那就加入的原则，以及对delta rule的信仰，我就在Mamba2的门控基础上，把Mamba2 (和vanilla linear attention) 用到的Hessian rule换成了Delta rule，从而把Mamba2变成了Gated DeltaNet，属于是经典A+B了。</p><p data-pid=\"RYtt9oon\">不过我倒是挺喜欢Gated DeltaNet的极简形式，虽说是A+B，但门控（A）和delta rule（B）已然是linear attention发展多年被提纯的两个最work的component，就应该狠狠的A+B。很庆幸他们的推广都有自己出的一份力</p><a href=\"https://www.zhihu.com/question/6098306614/answer/67036803631\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">2024年，你的科研工作进展如何？有什么心得体会想和大家分享？</a><p data-pid=\"dfclLt3Q\">在DeltaNet投稿之后，自己又想办法优化WY表示，能让它在GPU上运行的效率更高，最后「发明」了一个新算法，事后发现基本上就是重复造了UT transform的轮子。UT transform是一种WY表示的更适用GPU的等价变换， 并且UT transform可以用数学表达式把DeltaNet的attention形式用下三角矩阵求逆的形式很优雅的表示出来： </p><p data-pid=\"ClMtV_6g\"><img src=\"https://www.zhihu.com/equation?tex=A+%3D+%5Coperatorname%7Btril%7D%28QK%5ET%29%28I%2B%5Coperatorname%7Btril%7D%28KK%5ET%2C+-1%29%29%5E%7B-1%7D+\" alt=\"A = \\operatorname{tril}(QK^T)(I+\\operatorname{tril}(KK^T, -1))^{-1} \" eeimg=\"1\"/> </p><a href=\"https://www.zhihu.com/pin/1923340178266780376\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">《“对角+低秩”三角阵的高效求逆方法》…</a><p data-pid=\"6_obAuBI\">也算是提供了一种不一样的DeltaNet从<b>信息聚合</b>的图论视角的理解。同时，当时也受到了Forgetting Transformer这篇工作的启发，所以开启了一个新的工作。Forgetting Transformer本身的做法其实就是把Mamba2这类模型的data-dependent decay引入回了softmax attention，属于是线性注意力反哺softmax attention的一个典型。</p><a href=\"https://zhuanlan.zhihu.com/p/1923344184754026149\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">线性注意力简史：从模仿、创新到反哺</a><p data-pid=\"0udGgofQ\">当时自己正好琢磨出了上述的DeltaNet的attention写法，那么很自然的事就是不信邪，坚信delta rule的优越性，准备依葫芦画瓢，做一个(Gated) DeltaNet对应的softmax机制来counter Forgetting Transformer。</p><p data-pid=\"2rWuq7Rx\">这个idea看似容易，但做起来一点却一点不轻松。最开始的时候想把RoPE的rotation矩阵平替成data-dependent的householder矩阵，可以利用到orthogonal算子求逆等于转置的性质，用到类似RoPE的trick在QK上面做变换，之后直接调FlashAttention的kernel，把压力转移给tri dao的FlashAtention优化。</p><p data-pid=\"gmgUH-VM\">想法很美好，但是纸上得来终觉浅，实践的时候发现怎么做都做不work，因为这种norm-preserving的operator的累乘实在是太太太吃数值精度了（RoPE可以通过解析式算出rotation的累乘，不需要真的去算累乘），在低精度下面householder累乘是非常灾难的一件事。而fp32矩阵乘法的效率是完全不可以接受的，所以这个方案在多方尝试无果下只好放弃了。</p><p data-pid=\"GlNJPCAH\">好在，如果换成DeltaNet用到的那种允许部分contraction的Householder-like transition matrix的话，精度问题可以得到很好的改善，但不能利用上述的orthogonal算子的性质 reduce成flashattention的计算。 </p><p data-pid=\"rs046XXi\">最后被逼的没办法，只能自己硬推了一个跟FlashAttention写法类似的Blockwise PaTH计算的一个算法。这个算法也是非常优雅至极，巧妙的把block前向替换、householder累乘的UT transform、以及flashattention融合在了一起，自以为是突破了之前的DeltaNet chunkwise算法的高度，再创乐子新高了（x</p><p data-pid=\"C8NzE4sz\">但是backward梯度实在是难推+难写，用尽毕生算法所学（用了各种动态规划思想+重算来balance时间和空间复杂度），费了九牛二虎之劲才写出来，属于是PhD阶段做的最艰辛的一个project了，好在最后是做work了呜呜。这个工作算是自己非常喜欢的一个工作了，准备在这个夏天再做很多实验来让充分验证这个架构的潜力。</p><p data-pid=\"PpxLhGOU\">在此之中，发现同行们发掘出了DeltaNet的其他很多神奇的解释，比方说通过test-time training (TTT)的在线梯度下降的视角来理解DeltaNet。另外理论people也在发力，发现了deltanet可以突破 <img src=\"https://www.zhihu.com/equation?tex=TC%5E0\" alt=\"TC^0\" eeimg=\"1\"/> 这个表达力限制，从而可以更好地做state tracking。这两点不仅理论诱人，同时也有很多实际用途，比方说TTT框架在continual training和many-shots in-context learning的潜力，state tracking在coding 和reasoning task当中的潜力。</p><p data-pid=\"POR4JsQd\">既然DeltaNet既美丽又有实际关心的场景，那么我就可以放心大胆的来研究这个领域的乐子算法啦！后续可能想验证一下PaTH在coding和reasoning下的表现 </p><h3>Research philosophy </h3><p data-pid=\"cs3h_ddt\">从上面的回顾可以看出，其实自己的research strategy无非是（1）关注最fundamental的原理，（2）关注实际应用，做大家关心的问题，（3）工作之间连贯性很强，在不断的开拓自己的研究边界</p><p data-pid=\"KmPG8QvZ\">对于INTP来说，第一点就像呼吸一样自然，但第二点和第三点可能就需要多花点心思了。第二点的话就是得把控热点走向了：先看看大家关心什么问题，然后从自己的skill set里面捣鼓出一些来解决大家关心的问题的方案。像我喜欢做modeling的话方法论可能就是从第一性原理从模型架构的层面上来解决一些大家关心的东西，比方说之前提到了in-context recall和state tracking的问题，以及之后可能会设计一些架构来加强multi-hop reasoning的问题（有一个infini-hop attention的乐子idea等着我去实现ing）。</p><p data-pid=\"lohRAaD1\">对于第三点的话，自己也在有意识的稍微diversify一下自己的research，虽然还是在做modeling（毕竟做不了一点别的），但最近半年在避免自己做RNN，而是把精力放在了design一个更expressive的softmax attention上面，现在至少也算是平方和线性都做（四舍五入做的是1.5次方的attention），并且两者之间的关联非常密切，就像上文提到的用线性注意力的进展来反哺平方注意力。老板倒是很想让我多diversify一下自己的方向，比方说建议我从秋天开始换一个完全不同的方向来玩，但可能对我来说有点too much了，需要整理已有的work，很可能没什么精力来玩全新的东西。</p><h3>建立个人branding</h3><p data-pid=\"_yol3DQF\">建立自己的学术品牌也是一个老生常谈的问题了。我跟一个朋友的暴论就是，在AI research极度信息过载的今天，成为「网红」增大曝光度（visibility）是一个追求impact的PhD student的必由之路。要不然你的工作再好，不懂得宣传，也会面临酒在深巷无人问的窘境（除非你老板自带很多流量，你不用费吹灰之力就可以免费在公众号三大会登顶，在推特被AK疯狂转发等等）。 </p><p data-pid=\"W8QbcYLr\">感觉现在sonta的visibility在LM arch area还是挺高的，</p><p data-pid=\"uFL_KKwF\">形成的妙妙branding就是sonta名字在的arch paper都会非常有意思，比方说上个月一个神奇的转发带来了169K阅读</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-dab0ce794a470b32e9bc8da5655a7cd5_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb\" width=\"1230\" data-original=\"https://picx.zhimg.com/v2-dab0ce794a470b32e9bc8da5655a7cd5_r.jpg\" data-original-token=\"v2-dab0ce794a470b32e9bc8da5655a7cd5\"/><figcaption>看来国外读者也喜欢看乐子arch paper</figcaption></figure><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-e7fa9cea03a55bbd281dd32d8b79690a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"1252\" data-original=\"https://pica.zhimg.com/v2-e7fa9cea03a55bbd281dd32d8b79690a_r.jpg\" data-original-token=\"v2-e7fa9cea03a55bbd281dd32d8b79690a\"/><figcaption>感谢认可qwq</figcaption></figure><p data-pid=\"8icwTAw0\">然后被硬塞了一大波流量</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-85384f9baece7c2d7f0fd5dcbb292e53_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1316\" data-rawheight=\"1092\" class=\"origin_image zh-lightbox-thumb\" width=\"1316\" data-original=\"https://pic4.zhimg.com/v2-85384f9baece7c2d7f0fd5dcbb292e53_r.jpg\" data-original-token=\"v2-85384f9baece7c2d7f0fd5dcbb292e53\"/><figcaption>我也不知道为什么突然被cue了</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p data-pid=\"Eemkv7Ej\">推特粉丝直接从6K翻番到了近一倍到11K。</p><p data-pid=\"3pIypU_E\">目前的观感就是，visibility高的好处要远大于弊的（虽然sonta是现实的i人，但不妨碍自己做网上的巨人呀）。</p><p data-pid=\"QrZOrb7M\">下面就是自己提高visibility的一些方式：</p><h3>营业给Talk</h3><p data-pid=\"yxFrgsaq\">这个学期就是在疯狂营业给talk。起因大概率是因为博一的时候就被Richard Hamming的You and Your Reseach洗脑了，里面非常强调了宣传工作的必要性：</p><blockquote data-pid=\"22_Sx7xl\">“Many scientists and others think good ideas will win out automatically and need not be carefully presented. They are wrong; many a good idea has had to be rediscovered because it was not well presented the first time, years before!”</blockquote><p data-pid=\"Phh3pqkt\">并且建议拿出50%的时间来进行宣传</p><blockquote data-pid=\"XgwFss47\"> “I believed, in my early days, that you should spend at least as much time in the polish and presentation as you did in the original research. Now at least 50% of the time must go for the presentation. It’s a big, big number.”</blockquote><p data-pid=\"bxqa9f-c\">被Hamming洗脑的sonta 遂不断寻求营业的机会。今年年初刚好赶上DeepSeek R1的发布，对模型的推理速度带来了新的挑战（inference快才能更快的去做RL sampling），同时MiniMax01 hybrid model的发布也让人眼前一亮，可以肉眼感觉到业界对hybrid model以及linear attention的关心度在上升。</p><p data-pid=\"Wo8RSp3e\">在这个机遇下，年初的时候我就有想法开个talk series来科普这个方向的进展，先是在UW（线下）和UCSD（线上）先试水讲了一下，然后之后把slides分享在推特上，反响出乎意料的好，</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-021d339105dacc5f5ad4894175221965_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1264\" data-rawheight=\"794\" class=\"origin_image zh-lightbox-thumb\" width=\"1264\" data-original=\"https://picx.zhimg.com/v2-021d339105dacc5f5ad4894175221965_r.jpg\" data-original-token=\"v2-021d339105dacc5f5ad4894175221965\"/></figure><p data-pid=\"Gd4dPMHm\">在这个post之后也源源不断收到了很多invited talk的邀请。其中Sasha Rush也很喜欢这个slides的内容，邀请我到他的channel录了一期tutorial，</p><a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dd0HJvGSWw8A\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Linear Attention and Beyond</a><p data-pid=\"y9s1x-9T\">这个学期算下来感觉少说也讲了 20+ 个 talk，平均每周一个，横跨海内外的各种学界和业界，累成狗。和去年刚开始给 talk 时紧张得要命、要 mock 排练几次的状态比起来，现在只要有 slides 就能直接上（已经逐渐变成 talk 前几分钟还在赶 slides 的形状），不得不说自己确实成熟了不少（先不说讲得好不好，至少敢讲了，笑）。</p><p data-pid=\"SaCY2jfD\">彩蛋：最有记忆点的还得是FAI seminar的一个talk里面有观众问我有没有关心到Songlin Yang的工作  <a href=\"https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1Pg5JzEEad%3Ft%3D1565.5\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">bilibili.com/video/BV1P</span><span class=\"invisible\">g5JzEEad?t=1565.5</span><span class=\"ellipsis\"></span></a> （空降26min）</p><h3>营业ASAP seminar</h3><p data-pid=\"sJG9W1x6\">另外跟 @寒月灼华  好姐妹（？）一起开了一个fully virtual的seminar，叫做  <a href=\"https://link.zhihu.com/?target=https%3A//asap-seminar.github.io/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Advances in Sequence modeling from Algorithmic Perspectives</a> ，这个seminar名字一看就知道，很符合sonta的taste，不仅关心scaling，而且还关心（乐子）算法本身。 </p><p data-pid=\"FIBtYq9w\">之所以想做一个这样的seminar呢，是受到了<a href=\"https://link.zhihu.com/?target=https%3A//flann.super.site/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Seminars on Formal Languages and Neural Networks</a> 的启发，感觉对于一些前沿却不是很容易懂得概念，如果能请作者来讲一遍，然后把视频，slides和讨论全部公开的话，能够帮助萌新入行，为领域培养一些新鲜的血液。同时可以促进同行之间的交流，客观来看可以很大层面上促进这个领域的发展（以及能让sonta看到更多好玩的算法paper！）。</p><p data-pid=\"027J3ln7\">另外自己对没有recordings的seminar没有什么好感，因为很多这些talks对时间（不conflict）和空间（线下only）的要求很高，ADHD实在是没办法聚精会神听完整场talk，回放功能是刚需。</p><p data-pid=\"nXCgZum6\">之前就很想做一个这样的seminar，这个学期终于开始付诸行动，目前已经营业了26期（sonta精选乐子算法集），</p><a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/%40ASAPSeminarSeries\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">youtube.com/@ASAPSemina</span><span class=\"invisible\">rSeries</span><span class=\"ellipsis\"></span></a><p data-pid=\"jyHvSLPL\">两个主要主办方都是INTP，可想而知， P人营业 seminar 是没有固定时间的，想一出是一出（？），有时候一个星期两个，有时候直接 skip，总之办这个seminar还是很欢乐的一件事，并且很高兴有不少忠实观众收听，传播知识真是一件非常开心的事情（Fe在这种时刻大爆发）。</p><h3>营业FLA</h3><p data-pid=\"aAV9NpbU\">最后当然还是老生常谈的开源库。我很喜欢 <a href=\"https://link.zhihu.com/?target=https%3A//omarkhattab.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Omar Khattab</a> 在他的博客: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/okhat/blog/blob/main/2024.09.impact.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Impact</a>  里面提到的理念: Invest in projects, not papers。 把projects做成一个handy的开源库，papers只是在你搭建这个开源库过程中的一些side products。 这一点我相当buy，宁愿花时间多写库，也不想去水一些毫无意义的paper，因为在这个年头，papers are really cheap，一篇random paper<b>远远</b>没有一个well-maintained开源库的价值大。如果自己以后不幸做了PI，那么我肯定bias toward爱写库的小朋友，而不是有很多random paper的小朋友。</p><p data-pid=\"iWB99fNz\">这学期同样也花了很多精力来维护Flash Linear Attention。其实当时创建FLA这个开源项目的时候受到了 <a href=\"https://zhuanlan.zhihu.com/p/664061672\" class=\"internal\" target=\"_blank\">方佳瑞：大模型训练加速之FlashAttention系列：爆款工作背后的产品观</a> 强烈的启发：Tri Dao就是一个产品大师，亲手操刀了FlashAttention生态，以及后续Mamba生态的构建。Mamba的流行很大程度上也是归于他写了一个很好用的能直接pip install的库。各种牛鬼蛇神都可以一键把attention换成Mamba，在各种random任务上进行妙妙魔改，带来了一片「繁荣」。而一个反面教材就是RWKV，永远都在声称自己比同类模型好，有时间拉踩也没时间改进自己仓库的易用度，那自然是流行程度有限。</p><p data-pid=\"UMoxWjSq\">最近半年FLA对varlen/batch inference以及CI自动化测试的支持力度加大（感恩 <a class=\"member_mention\" href=\"https://www.zhihu.com/people/8d5f4c3b705ef7df799b680515700468\" data-hash=\"8d5f4c3b705ef7df799b680515700468\" data-hovercard=\"p$b$8d5f4c3b705ef7df799b680515700468\">@yzhangcs</a> @zhiyuan1i），得到了友人兼FLA老用户的认可，深感欣慰。</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-e0a1d4fddae00cee4ae2d702f31c546b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"402\" data-rawheight=\"348\" class=\"content_image\" width=\"402\" data-original-token=\"v2-e0a1d4fddae00cee4ae2d702f31c546b\"/></figure><p data-pid=\"-e-RfX8A\">有人喜欢FLA，那自然也有不喜欢FLA的，比方说上个月的纷争，在这里我就不多谈了，大家还是来看武侠小说来乐把</p><a href=\"https://zhuanlan.zhihu.com/p/1915601328211759191\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">《线性注意力恩仇录》</a><h2>Misc.</h2><p data-pid=\"JZFweldQ\">下面就是各种（奇怪的）感悟和流水账</p><h3>没有救的作息+INTP应该投资住的地方</h3><p data-pid=\"KrTOcQ1F\">这个学期由于选了0门课，加上基本没有早会，在没有任何约束的情况下，作息就仿佛自由落体般在晚睡晚起的路上一路狂奔，从最开始的平均3-10的作息到了5-12，甚至常常7点才睡觉，见到了很多很多次Cambridge的清晨，不知道熬了多少个通宵。</p><p data-pid=\"efnOD0yv\">学期中感觉浑浑噩噩的，后知后觉才意识到可能跟居住环境有一定关系。这个学年在Ashdown的一楼studio住了一年，但感觉居住体验很差，一是房间空间太小住着不舒服，其次就是一楼完全不想开窗，索性就常年把帘子全拉上，完全失去日常光，literally小黑屋了属于是。</p><p data-pid=\"FAEcP52V\">小黑屋对人的影响还是挺大的：因为房间全部是黑乎乎的，所以失去了自然光照的调节，很容易睡到中午或者下午，并且睡醒的那段时间特别没精神（应该是少了自然光照调节的warmup？），以及失去光照对人的情绪的影响还是不小的。</p><p data-pid=\"NVCHpp3c\">痛定思痛之后决定赶紧把这破房间退了，打死不续住 （虽然抽到了可以续的签）。summer则是租了个site4的公寓来过渡，顿时就感觉到了居住体验的差别：site4 view很好高层落地窗很明亮，房间也很新，而且交通很便利就在Kendall旁边，住起来舒服多了（虽然最开始差点被不能调温度的中央空调冻死，但似乎已经习惯了）。</p><p data-pid=\"SEnGhhcf\">因为INTP大宅b绝大多数时间都在家里，居住环境的体验还是影响很大的。之前的我只意识到自己无论如何也要住studio，但似乎对具体的条件没啥要求，现在已经感觉到有需求了，起码需要空间大一点的东西来容纳本ADHD随手往地上扔的东西（x）</p><h3>Overcommit</h3><p data-pid=\"3EBbWWGP\">ADHD很容易overcomit。 上一篇随笔就提到了自己的overcommit的倾向，但是这个学期似乎不降反增，在学期初的时候答应了一堆合作，其中有个做蛋白质modeling的老哥格外热情，被拉着开了几次会后感觉也没啥兴趣索性ghost了事（很坏了，不要学，但我也不知道咋拒绝掉，所以还是ghost吧）。一个常见的over-commit的pattern就是答应帮人优化kernel（觉得自己写kernel既快又好是吧）。比方说本来跟mesanet的团队说COLM之后来优化，然后拖到了NeurIPS后他们要arxiv的时候才连忙来优化。NeurIPS之后还兑现了一下对DeltaProduct优化的promise（promise了小半年），基本上6月份就是在用来还overcommit的债。已经体验到痛了，下次不许了。</p><h3>Limerence与皮肤饥渴</h3><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-dbaf2e308dfe6d7a16530cf52ebdb05b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1050\" data-rawheight=\"1090\" class=\"origin_image zh-lightbox-thumb\" width=\"1050\" data-original=\"https://pic2.zhimg.com/v2-dbaf2e308dfe6d7a16530cf52ebdb05b_r.jpg\" data-original-token=\"v2-dbaf2e308dfe6d7a16530cf52ebdb05b\"/></figure><p data-pid=\"qVYUTsZG\">ADHD由于缺乏多巴胺，所以很容易陷入crush。在短短几个月经历过两次类似的情感体验后，意识到自己不能在这个cycle里面反复循环了，现在在crush前会警惕拉满，以免重蹈覆辙）</p><p data-pid=\"oXy9FOVu\">期间也在试探自己的性向，但发现自己也没有自己想象的那么拉子，被朋友瑞萍道：sonta只不过是需要skinship的无性恋罢了。细想了一下，应该的确如此。在雌性激素的作用下，对贴贴抱抱的需求很高，经过反复的探索，发现还是喜欢肢体接触的直女（微拉）朋友最能满足情感需求，sonta需要的原来不是crush而是闺蜜。线下捕获sonta的话，欢迎来贴贴和抱抱！sonta真的很需要</p><h3>Traveling</h3><p data-pid=\"v5zI0j3D\">这学期刷了两次芝加哥和三次纽约。2月份来找ttic的朋友们玩（顺便去ttic给了个talk） 体验到了芝伯利亚的2月极寒，不服气5月份neurips投稿之后又来体验了下芝加哥的初夏。芝加哥的downtown非常的干净漂亮，在住在downtown的朋友家美美宅了几天+周边小玩，恢复精气神，但是回去的时候体验到了芝加哥这个城市不负最堵城市之名，首先是思想出了问题买了ORD机场的票（离市区更远），其次是思想出了大问题4点多的时候打uber而不是坐火车去ORD被堵在高速不能动弹（这个时候才想起来一个朋友告诉我说她每次至少提前3个小时去ORD的理由了），喜提免费改签。</p><p data-pid=\"Nk-v4oIB\">去了三次纽约，每次都赶上下雨，也是绝了。第一次是四月初的时候去第一次看解解演唱会，去了演唱会才知道原来还有新的心跳+水上漂+夜的尽头 的蹦迪环节，属于是伪粉丝了（不过这几首蹦迪曲是真的好听啊呜呜呜，回家直接循环了这几首歌一个月）。本想着周六早上去，COLM反正周五早上截稿，谁知COLM延了一天，通宵赶稿（还没赶上）直接转场去赶车的体验实在太酸爽了（没死在车上不错了）。回来的时候又碰上大暴雨，火车晚点了几个小时，很难过了（不过@寒月灼华 飞机直接延误到通宵候机还是很让我暖心的）。</p><p data-pid=\"fpEeJxGU\">第二次五月初去纽约参加JaneStreet Fellowship Workshop，去年有点太摆了错过了几乎所有的tech厂的fellowship，于是申请了两个deadline晚的quant家的fellowship：2sigma和janestreet。2sigma要学校推荐，虽然学校推了，但我忘记交材料了，直接寄了。JS就变成唯一的稻草了，不过好在最后这个米还是恰到了。本来不太想去JS的workshop，因为时间有点阴间，一是时间在NeurIPS截稿前一周多点，正处于奋斗奋斗还能交的上的关键节点，怎么能来workshop呢？二是JS的人不看黄历，workshop选的时间刚好是开始的那天开始下雨，下到结束那天停，于是行程一环的欣赏纽约最高最好的view变成了看窗户上的雾滴和白茫茫的窗外（乐）。</p><p data-pid=\"A9fSDYHs\">但是想摆的念头还是被老板打消了了，没想到老板也打算去JS的这个workshop。 老板表示我总不能让他一个人在workshop茕茕孑立社恐发作吧，那得 我还是去吧。不过去了还是不后悔的，JS还是太有钱了呜呜吃住还是挺舒服的，还认识到了一些人，虽然绝大多数时间都没怎么出去玩就是了，一是下雨，二是真的要赶稿，开启了酒店赶稿模式，只参加了必要的营业环节（给talk+领奖）</p><p data-pid=\"piJpiKzY\">第三次是六月初的时候四个INTP姐妹从波士顿一起去纽约旅游。大家都知道INTP旅游的含金量，更不用说4个人了。第一天11点的bus我最后0min才赶到，四个人在station面面相觑在思考车在哪里，然后看了下邮件才发现发车时间改了，但是0个INTP看了邮件。到了纽约之后发现朋友在xhs联系的(不靠谱的)留子的apartment没法入住，因为联系不到人，直到晚上10点才回了一句人刚从NYPD警察局出来（什么剧本？？），今天没法让我们住，可以给我们退钱，于是四个人开启了临时找酒店之旅但是0个人在抱怨（感觉J人应该已经被气死了）。4个intp没有任何计划安排的玩了3天，主要就是吃吃喝喝，还是很欢乐的</p><h3>Summer internship</h3><p data-pid=\"eBlY19NE\">去年实习学到的最大lesson就是不要去一个random、自己不了解、没有熟人的组，否则开盲盒踩坑的概率相当的大，因为业界有爹味的混子真的很多， 而INTPs是真的一丁点都不服管，尤其是不服毛也不懂的人在瞎指挥。</p><p data-pid=\"tNHSNJdy\">去年的不爽遭遇导致今年出去找实习欲望暴跌，再加上第一梯队的公司要么publication policy在急剧收紧 (e.g., Google DeepMind)，要么实习生碰不到核心东西。第二梯队的公司往往混子横行，去了一般就是技术扶贫，对自己的成长没有半点帮助，虽然有无数公司reach out，但自己还是想留在BOS开大摆。老板也有意让自己留在BOS，索性就在MIT-IBM Waston AI Lab挂了个实习，毕竟平常学期中的时候也是跟这个组在深度合作，所以实习拥有绝对的自由，领着业界的钱，同时还可以带可爱的小朋友做Research，还是挺美滋滋的。本来想着体验波士顿美好的夏天，但是感觉波士顿的夏天有点剧毒。先是连着N个周末下了雨，然后进入暴晒模式，有一天来到了历史温度最高，可以说是非常宜人了（x</p><h3>业界 vs. 学界?</h3><p data-pid=\"Ia6ObO_C\">学术界的吸引力在肉眼可见的急剧变小。如果是西海岸两所的基本操作是quit创业，那么MIT这边流行三年速通PhD润业界。虽说其实自己做的东西跟业界挺搭的，但自己的兴趣点在开源，以及分享知识，感觉并不是特别想给闭源公司打工，更加倾向学术界一点点。之前自己问simran为什么chris re组的phd很多都选择了找教职，她说他们已经看多了startup到免疫了，反而是想在学术界做一些更加有创造性的新的东西，而不是去业界重新造一遍别的地方已经造过的轮子。虽然自己更倾向学界一点，但是如果能有地方能给我足够多的自由度和卡来让我玩自己的乐子架构的话，没准我还是很有可能被业界拐跑的 ：（ 感觉自己处在一个微妙的摇摆位置 ）</p><h3>尾记</h3><p data-pid=\"HRXkyRjl\">下下周会在温哥华参加ICML，欢迎线下捕捉）</p>",
        "excerpt": "本来想着五月底写完的，然后写了一半就不幸开摆了。这次趁着独立日假日来一口气写完前期回顾传送门： <a href=\"https://zhuanlan.zhihu.com/p/674624059\" class=\"internal\" target=\"_blank\">sonta：PhD第一学期 年末随笔</a> <a href=\"https://zhuanlan.zhihu.com/p/702482919\" class=\"internal\" target=\"_blank\">sonta：PhD 第二学期随笔</a> <a href=\"https://zhuanlan.zhihu.com/p/12745534263\" class=\"internal\" target=\"_blank\">sonta：PhD 第三学期年末随笔</a>不知不觉已经到了这个系列的第四集啦，这一集至少是整个系列的中场，因为自己并没有读满五年的打算。 既然是一个整体的中场时刻，那么在这篇中，会有很多总结以及回顾向的内容，用来阐述自己在Research哲学以及心路历程，和一些生活感悟，hop…",
        "excerpt_new": "本来想着五月底写完的，然后写了一半就不幸开摆了。这次趁着独立日假日来一口气写完前期回顾传送门： <a href=\"https://zhuanlan.zhihu.com/p/674624059\" class=\"internal\" target=\"_blank\">sonta：PhD第一学期 年末随笔</a> <a href=\"https://zhuanlan.zhihu.com/p/702482919\" class=\"internal\" target=\"_blank\">sonta：PhD 第二学期随笔</a> <a href=\"https://zhuanlan.zhihu.com/p/12745534263\" class=\"internal\" target=\"_blank\">sonta：PhD 第三学期年末随笔</a>不知不觉已经到了这个系列的第四集啦，这一集至少是整个系列的中场，因为自己并没有读满五年的打算。 既然是一个整体的中场时刻，那么在这篇中，会有很多总结以及回顾向的内容，用来阐述自己在Research哲学以及心路历程，和一些生活感悟，hop…",
        "preview_type": "default",
        "preview_text": "",
        "url": "https://zhuanlan.zhihu.com/p/1906936848632256357",
        "comment_permission": "all",
        "voteup_count": 893,
        "comment_count": 55,
        "image_url": "",
        "linkbox": {
          "url": "",
          "category": "",
          "pic": "",
          "title": ""
        },
        "reaction_instruction": {},
        "upvoted_followees": [],
        "voting": 0,
        "content_need_truncated": false,
        "force_login_when_click_read_more": false,
        "is_navigator": false,
        "navigator_vote": false,
        "vote_next_step": "vote"
      },
      "verb": "MEMBER_VOTEUP_ARTICLE",
      "created_time": 1752309907,
      "interaction": {
        "can_show_sticker": true
      },
      "actor": {
        "id": "ac113c7837148e5d43c18a06cf248d21",
        "name": "河畔草lxr",
        "headline": "",
        "type": "people",
        "user_type": "people",
        "url": "https://www.zhihu.com/people/liuxiaoran-34",
        "url_token": "liuxiaoran-34",
        "avatar_url": "https://picx.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_l.jpg?source=5a24d060&needBackground=1",
        "gender": 1,
        "is_following": false,
        "is_followed": false,
        "is_org": false,
        "badge": [],
        "badge_v2": {
          "title": "",
          "merged_badges": [],
          "detail_badges": [],
          "icon": "",
          "night_icon": ""
        },
        "vip_info": {
          "is_vip": false,
          "target_url": ""
        },
        "kvip_info": {
          "is_vip": false,
          "target_url": ""
        }
      },
      "action_text": "赞同了文章",
      "is_sticky": false
    },
    {
      "id": "1748928612918",
      "type": "feed",
      "target": {
        "id": "1910672718174589774",
        "type": "answer",
        "url": "https://api.zhihu.com/answers/1910672718174589774",
        "voteup_count": 1325,
        "thanks_count": 33,
        "question": {
          "id": "1908479621466396378",
          "title": "如何评价Google刚刚发布的 Gemini Diffusion? 会代替自回归模型成为下一代模型吗？",
          "url": "https://api.zhihu.com/questions/1908479621466396378",
          "type": "question",
          "question_type": "normal",
          "created": 1747796999,
          "answer_count": 37,
          "comment_count": 0,
          "follower_count": 679,
          "detail": "<p><a href=\"https://link.zhihu.com/?target=https%3A//deepmind.google/models/gemini-diffusion/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gemini Diffusion - Google DeepMind</a></p>",
          "excerpt": "<a href=\"https://link.zhihu.com/?target=https%3A//deepmind.google/models/gemini-diffusion/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gemini Diffusion - Google DeepMind</a>",
          "bound_topic_ids": [
            5231,
            1963575,
            2450152,
            3978151
          ],
          "relationship": {
            "is_author": false
          },
          "is_following": false,
          "author": {
            "id": "d2240cf1c3ac1bd2ada3d2baf0d9efdd",
            "name": "虚无",
            "headline": "喜欢各种科学。huanranchen.github.io",
            "type": "people",
            "user_type": "people",
            "url": "https://www.zhihu.com/people/xu-wu-60-26",
            "url_token": "xu-wu-60-26",
            "avatar_url": "https://picx.zhimg.com/v2-ae9623faaf5f54095ea9859cc8e00bcb_l.jpg?source=5a24d060&needBackground=1",
            "gender": 1,
            "is_following": false,
            "is_followed": false,
            "is_org": false,
            "badge": [],
            "badge_v2": {
              "title": "",
              "merged_badges": [],
              "detail_badges": [],
              "icon": "",
              "night_icon": ""
            },
            "vip_info": {
              "is_vip": false,
              "widget": {
                "url": "https://pic1.zhimg.com/v2-76671fb4e71201e504f38de327567f5e_r.jpg?source=5a24d060",
                "night_mode_url": "https://picx.zhimg.com/v2-b377040f3ee91bb20cf3a7bfec54f6f2_r.jpg?source=5a24d060",
                "id": 12004
              },
              "target_url": ""
            },
            "kvip_info": {
              "is_vip": true,
              "vip_icon": {
                "url": "https://pic1.zhimg.com/v2-57fe7feb4813331d5eca02ef731e12c9_r.jpg?source=5a24d060",
                "night_mode_url": "https://pic1.zhimg.com/v2-57fe7feb4813331d5eca02ef731e12c9_r.jpg?source=5a24d060"
              },
              "target_url": "https://www.zhihu.com/kvip/purchase"
            }
          },
          "reaction_instruction": null
        },
        "updated_time": 1748392909,
        "created_time": 1748319874,
        "author": {
          "id": "ecf8a3dae5a0210126cab5039646e338",
          "name": "李崇轩",
          "headline": "生成模型",
          "type": "people",
          "user_type": "people",
          "url": "https://www.zhihu.com/people/li-chong-xuan-42",
          "url_token": "li-chong-xuan-42",
          "avatar_url": "https://picx.zhimg.com/v2-3ee3b812c8f6917ac3f9fdef0ae6c368_l.jpg?source=5a24d060&needBackground=1",
          "gender": 0,
          "is_following": false,
          "is_followed": false,
          "is_org": false,
          "badge": [],
          "badge_v2": {
            "title": "",
            "merged_badges": [],
            "detail_badges": [],
            "icon": "",
            "night_icon": ""
          },
          "vip_info": {
            "is_vip": false,
            "target_url": ""
          },
          "kvip_info": {
            "is_vip": false,
            "target_url": ""
          }
        },
        "comment_permission": "all",
        "is_copyable": true,
        "comment_count": 62,
        "can_comment": {
          "status": false,
          "reason": "只有作者关注的人才可以评论"
        },
        "content": "<p data-pid=\"mmvnjMMa\">大家好，我是中国人民大学高瓴人工智能学院李崇轩，因为做的非常相关，来回答一下这个问题。</p><p data-pid=\"vq0oYJUs\">我在连续扩散模型和朱军老师以及师弟师妹们有很多合作，代表性工作有 Analytic-DPM，U-ViT, DPM-Solver，ProlificDreamer，DPM-Solver++，unidiffuser 等等。</p><p data-pid=\"Bk4naQiN\">我在人大的课题组很年轻，组内在离散扩散模型的代表性工作有 RADD，Scaling Law for MDM，LLaDA，LLaDA-V 和这两天即将发布的 LLaDA 1.5。</p><p data-pid=\"luAJJ7Xb\">我想可以按照时间划分为两个阶段来介绍一下这个领域，然后发表一下我的看法。</p><h2>第一阶段：2022-2024年底，扩散语言模型偏基础研究的阶段。</h2><p data-pid=\"wnAAmtsh\">这个阶段其实扩散语言模型是比较偏小众的，我按照离散和连续模型的区别来划分。</p><h3>1.1 连续扩散模型在离散数据上的发展 </h3><p data-pid=\"K_gV5NLc\">早期有不少方法是通过将连续（即基于 Gaussian 加噪去噪的）的扩散模型直接应用到语言模态上，一定程度可以理解为对离散数据做了连续化。代表性的工作有 Percy Liang 的，Ting chen 和 Hitton 合作的，quanquan gu和 zaixiang zheng，还有 Alex Graves 的  Bayesian flow network，另外还有苹果的 Yizhe Zhang 以及港大的 Lingpeng Kong 课题组的非常多的工作（如果有忘记的工作不好意思，因为自己纯凭印象难免有疏漏，请在评论区补充哈）。</p><p data-pid=\"pi3Gq901\">其他采用连续模型，但是避免离散数据连续化的研究思路也有，比如 lou 和 ermon 做过一些离散数据概率分布的参数空间（单纯形空间）受约束下的连续扩散模型，这种技术性更强，但是可能可扩展性和采样方法更困难。</p><p data-pid=\"mvXrlMEy\">另外一个思路是 ICML 2024 的一个工作，做过一个通过 SDE 统一 BFN 和 diffusion model 的方法，可以理解为，在logits 空间（过softmax之前）做 linear SDE，这样不需要连续化数据，也不需要受到单纯形的约束，还能直接把 DPM-solver 等 recipe 拿过来做加速，我觉得从方法上来讲是很漂亮的。</p><p data-pid=\"eJeFBXtj\">我们组其实在扩散语言模型的研究阶段起步不是很早，我们大概在 2023年6月开始做，上述 ICML 2024 就是我们第一个相关工作，我个人选择连续的扩散模型的理由是：因为这样可以无缝多模态。语言，图像，视频直接概率建模方式统一为扩散，基于底层一个 transformer 模型（类似 unidiffuser）可以大一统。</p><p data-pid=\"hjmeXSN8\">然而，很不幸，目前来看这类方法在语言上的可扩展性是存疑的，如 Plaid 文章中有一些结果是相对负面的。但是，最近BFN在科学等小规模数据上应用非常广泛，大家可以关注一些子刊的文章。</p><h3>1.2 离散扩散模型</h3><p data-pid=\"UUTDOPLv\">追根溯源，离散扩散模型其实在 ICML 2015 diffusion 刚提出来的时候就有了。 Austin 和 Jonathon Ho 等人 21 年的 D3PM 算是比较现代的 MDM 的雏形，但是 loss function 有点 tricky。Lou、<span class=\"nolink\">M</span>eng 和 Ermon 的  SEDD 提出了一种 EBLO 的优化损失，拿到了 ICML 的杰出论文，（中间 MDM 的工作还蛮多，推荐大家去看看相关文章的 related work 吧，可以关注下 DeepMind Jiaxin Shi, 康奈尔 <span class=\"nolink\">Volodymyr Kuleshov</span> ）。</p><p data-pid=\"5gYMvaKO\">离散扩散模型的建模并不只是一种。其中比较有名的一种是平稳分布为词表上的均匀分布，称为 Uniform，另外一种是全部掩码的，称为 MDM（Maksed Diffusion Model）。当时基于多个工作的实验，都发现 MDM 的结果比 Uniform 好很多。刚好那个时候大家可能也发现连续的扩散模型 scaling 不行。因此，大家的重心自然地会转向 MDM。</p><p data-pid=\"BMP08SZN\">我们最初还是希望系统地理解 MDM 的本质。我们在 MDM 上的第一个工作结论是让我非常惊讶的：一个 MDM 的转移核，应该是一个关于 t 的函数（大家想想连续的扩散模型都需要把时间 t 输入网络去噪），但是我们证明了 MDM 中这个函数关于 t 的部分是一个有解析解的标量，乘在一个需要学习的模型分布上。那么，我们自然地可以重参数化，把 t 从模型输入中拿掉，最后在输出上乘上去，这就是 RADD，基于 RADD 可以简化很多 SEDD 的模型和损失函数设计。例如，RADD 去掉 t 意味着我连 Diffusion Transformer 都不需要，直接用 Transformer 就好了，那么自然地就跟 BERT、MaskGIT 建立了联系。</p><p data-pid=\"SRduAaoY\">2024 年 6 月我们的 RADD 工作放到 arxiv 上（Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data），同期还有两个工作，分别是我的师弟 Deepmind jiaxin shi（同一天很巧），还有 康奈尔 <span class=\"nolink\">Volodymyr Kuleshov</span> （晚一点）组的。这三个工作损失函数非常像，我们比较独有的贡献是证明了输入中 t 应该去掉，佳欣做了 schedule 的一般化和图文的实验，<span class=\"nolink\">Volodymyr Kuleshov</span> 实验验证了去掉 t 是有效的，并且做了最好的语言的结果。</p><p data-pid=\"0jHjMEkA\">2024 年 9 月，Kaiwen 的工作放在 arxiv 上 （Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling ）也证明了可以去掉时间 t，并特别指出了 MDM 似然实验中的数值问题。</p><h3>1.3 和 BERT 和 MaskGIT 的关系</h3><p data-pid=\"OTBeLteV\">因为这个问题特别受到关注，我就单独回答一下。MDM 和 BERT 的关系如下：</p><p data-pid=\"FYSmmoS-\"><b>MDM -&gt; 采用 mask 转移核（不能用 uniform 的）-&gt; 证明其时间 t 在输入中是冗余的 （不然带着 t 用的是 DiT/U-ViT）-&gt; 输入随机掩码改为固定掩码（直接不是一个生成模型了）-&gt; BERT</b></p><p data-pid=\"VETTcR8R\">我们组和非常多 MDM 的人是机器学习出身，特别是 generative model 出身，最初的动机包括 MDM 本身的发展是沿着概率建模的思路的，因此就顺着叫 Diffusion 了（如果往前追到 ICML 2015， 这比 BERT 还早），后来我们确实发现了和 BERT 以及 MaskGIT 的联系，也都在 related work 提到了，并没有人回避这个问题。</p><p data-pid=\"rYOpP_lB\">但是二者上述的区别我认为是非常显著的，<b>技术上，BERT 的联合概率是怎么定义的？怎么从BERT 里面采样文本？如果没有这些能力，作为一个非生成模型，BERT 怎么跟 GPT 一样统一语言任务呢？有很多 Scaling BERT 的工作，为什么他们只作为 Embedding model 不做基础生成模型？谷歌 BERT 积累这么多年，为什么今天 Gemini Diffusion 才出来？</b></p><p data-pid=\"eaaUPszl\">其实具体叫啥名字也没有那么重要，称 MDM 为 <b>Modified/Randomized BERT 技术上应该也没错，但是把 Modified/Randomized 去掉，说就是 BERT，是不是不太客观？</b></p><p data-pid=\"bG9sgRGh\"><b>我个人坚持认为 Diffusion 是更合适的名字，这是因为一路发展就是这么过来的，技术上就是扩散，并且能突出它是一个生成模型，因为他是生成模型我们才相信他能 scaling 后跟 GPT 打擂台，而不是因为它像 BERT。</b></p><h2>第二阶段：2024年底-2025年初，扩散语言模型偏 scaling 的阶段。</h2><h3>2.1 学术界工作</h3><p data-pid=\"NBuDmJO_\">RADD 出来之后，我觉得已经非常清楚了，就决定投入我们组 5 位博士生，一半的人力到 MDM 方向。2024 年我开启动小组会的时候跟大家说，我们有<b>三个目标：</b></p><ol><li data-pid=\"32zhBSz7\">我保证给每一位同学找到合适的 MDM 项目，进行系统科研训练，然后每个人都有自己一作的工作（我已经做到了），不耽误大家前程；</li><li data-pid=\"e490ffrR\">我们要团结起来，课题组一起做一些有影响力的工作（这就是 LLaDA 系列）；</li><li data-pid=\"Ku_eV8hM\">解决 MDM 最难的问题，让顶尖的公司在端侧或者云侧上线 MDM 模型或者技术，干掉 ARM（划掉）。</li></ol><p data-pid=\"vQ_2UVnC\">就是在这三个目标的牵引下，我的课题组和合作者们开始从 MDM 的基础研究转向 Scaling。</p><p data-pid=\"yWmQfjoc\">第一个工作是 2024 年 10 月， 课题组做了第一个 MDM 的 Scaling Law（ICLR 2025）。并且，考虑到似然评估的数值问题（之前提到了）和大规模应用中下游任务的重要性，我们系统评测了 MDM 和 ARM 的表现。实验规模比较小，但是数据、算法、模型、计算量全部对齐的。其结论解决了我们认为当时最重要的问题：<b>从头训练，GPT-2 大小下，MDM 不亚于自回归。</b></p><p data-pid=\"eLZs-sGz\">第二个工作是 2025 年 2 月，课题组做了第一个能够多轮对话的 MDM，LLaDA 8B。LLaDA 展示 了很好的可扩展性，指令追随等能力，能够和 LLaMA 3 8B 这种代表性工作媲美。这样我们更加坚信这条路是值得探索的。LLaDA 做完之后，课题组经过了讨论，决定全力支持开源工作，把一切能开源的代码和权重全都放了出去，并且尽快地回复社区内的所有问题。我们希望有更多的人加入领域，完善MDM，这样才有机会完成目标 3。</p><p data-pid=\"A2UDR0zw\">后续的发展也确实像我们预想的一样，越来越多人关心这个领域。近期，在语言对齐和推理相关的工作有： d1, LLaDou, 还有我们最近会放出去的 LLaDA 1.5（VRPO）；在多模态相关的工作有：王梦迪老师的 MMaDA，LaViDA，和我们的 LLaDA-V；在diffusion上做类似 kv cache 的加速技术也有两篇工作 dimple 和 dLLM-cache。</p><p data-pid=\"MEkpEhVm\">另外，除了从头 Scaling，还有一条路是  Lingpeng Kong 课题组做的基于自回归模型微调的 diffuLLaMA 和 Dream。</p><h3>2.2 工业界工作</h3><p data-pid=\"38E6_DTC\">LLaDA 2月份开源后，Inception Lab 也火速上线了 Mercury coder，5月份 Gemini Diffusion 亮相也引起了更多的关注。但是，我们做 LLaDA 并没有受到这些产品的任何启发，看时间线就知道。</p><p data-pid=\"at9RTWAT\">工业界的产品我觉得更方面肯定会比学术界做的更好，他们做的一些加速效果我个人觉得是真的，可以用cache，可以跳步，也可以蒸馏。不过很遗憾目前二者都没有透露太多的技术细节。</p><h3>关于 LLaDA 的贡献和局限</h3><p data-pid=\"Q1u6ZEgH\">LLaDA 是我做过的最好的工作，即便被 ICML 拒稿，我从2014年读博开始做 generative model 十一年，做过 vae，做过 gan，做过 ebm，做过 diffusion。终于有一次，是我告诉别人这个东西是 work 的，而不是别人告诉我。有人质疑说 LLaDA 没有新的技术，说实话生成模型偏技术方面的工作我做过不少，Analytic-DPM 就很技术；应用的我也做过不少，U-ViT 就非常直接。我认为不管有没有技术，最好的研究是改变/加深大家对一个东西的理解和认知，我认为做 LLaDA 过程中就改变了我很多的认知。</p><p data-pid=\"mluu_2wD\">如果你觉得 LLaDA 没有改变你的认知。可以问一问自己，时间回退一年，在 Diffusion 做 work 之前，你觉的这条路会成吗？给你 300 张卡，只有一个半月的使用时间，这是你能争取到最大的资源，你会用它做什么？</p><p data-pid=\"2Mb7yFtH\">很多人提到了变长等问题，我觉得非常敏锐，这是一个很难的、值得研究的问题。特别是对做研究的朋友们而言，如果一个问题大家都觉得简单，那可能很卷了。扩散语言模型还是有非常多值得探索的空间，机会就在困难之中。</p><p data-pid=\"SygOOkSD\">写的有点长了，累了。也欢迎关注我们课题组 2 月份发的知乎文章 <a href=\"https://zhuanlan.zhihu.com/p/24214732238\" class=\"internal\">GSAI-ML：自回归是否是通往语言智能的唯一路径？——生成模型的一些思考（3）</a></p>",
        "excerpt": "大家好，我是中国人民大学高瓴人工智能学院李崇轩，因为做的非常相关，来回答一下这个问题。 我在连续扩散模型和朱军老师以及师弟师妹们有很多合作，代表性工作有 Analytic-DPM，U-ViT, DPM-Solver，ProlificDreamer，DPM-Solver++，unidiffuser 等等。 我在人大的课题组很年轻，组内在离散扩散模型的代表性工作有 RADD，Scaling Law for MDM，LLaDA，LLaDA-V 和这两天即将发布的 LLaDA 1.5。 我想可以按照时间划分为两个阶段来…",
        "excerpt_new": "大家好，我是中国人民大学高瓴人工智能学院李崇轩，因为做的非常相关，来回答一下这个问题。 我在连续扩散模型和朱军老师以及师弟师妹们有很多合作，代表性工作有 Analytic-DPM，U-ViT, DPM-Solver，ProlificDreamer，DPM-Solver++，unidiffuser 等等。 我在人大的课题组很年轻，组内在离散扩散模型的代表性工作有 RADD，Scaling Law for MDM，LLaDA，LLaDA-V 和这两天即将发布的 LLaDA 1.5。 我想可以按照时间划分为两个阶段来…",
        "preview_type": "default",
        "preview_text": "",
        "relationship": {
          "voting": 0,
          "is_thanked": false,
          "is_nothelp": false,
          "upvoted_followee_ids": null
        },
        "reaction_instruction": {},
        "relevant_info": {
          "relevant_type": "",
          "is_relevant": false,
          "relevant_text": ""
        },
        "reshipment_settings": "allowed",
        "answer_type": "normal",
        "paid_info": {
          "type": "",
          "content": "",
          "has_purchased": false
        },
        "attachment": null,
        "is_navigator": false,
        "navigator_vote": false,
        "vote_next_step": "vote",
        "mark_infos": [],
        "content_need_truncated": false,
        "force_login_when_click_read_more": false
      },
      "verb": "MEMBER_VOTEUP_ANSWER",
      "created_time": 1748928612,
      "interaction": {
        "can_show_sticker": true
      },
      "actor": {
        "id": "ac113c7837148e5d43c18a06cf248d21",
        "name": "河畔草lxr",
        "headline": "",
        "type": "people",
        "user_type": "people",
        "url": "https://www.zhihu.com/people/liuxiaoran-34",
        "url_token": "liuxiaoran-34",
        "avatar_url": "https://pica.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_l.jpg?source=5a24d060&needBackground=1",
        "gender": 1,
        "is_following": false,
        "is_followed": false,
        "is_org": false,
        "badge": [],
        "badge_v2": {
          "title": "",
          "merged_badges": [],
          "detail_badges": [],
          "icon": "",
          "night_icon": ""
        },
        "vip_info": {
          "is_vip": false,
          "target_url": ""
        },
        "kvip_info": {
          "is_vip": false,
          "target_url": ""
        }
      },
      "action_text": "赞同了回答",
      "is_sticky": false
    },
    {
      "id": "1740674306790",
      "type": "feed",
      "target": {
        "ab_config": {
          "md_interact_unify": "1"
        },
        "admin_closed_comment": false,
        "author": {
          "avatar_url": "https://picx.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_720w.jpg?source=8a6f5038&needBackground=1",
          "badge_v2": {
            "detail_badges": [],
            "icon": "",
            "merged_badges": [],
            "night_icon": "",
            "title": ""
          },
          "exposed_medal": {
            "avatar_url": "",
            "description": "",
            "medal_avatar_frame": "",
            "medal_id": "0",
            "medal_name": "",
            "mini_avatar_url": ""
          },
          "follower_count": 641,
          "gender": 1,
          "headline": "",
          "id": "ac113c7837148e5d43c18a06cf248d21",
          "is_advertiser": false,
          "is_blocked": false,
          "is_blocking": false,
          "is_followed": false,
          "is_following": false,
          "is_org": false,
          "is_special_follow": false,
          "kvip_info": {
            "is_vip": false,
            "target_url": ""
          },
          "name": "河畔草lxr",
          "reaction_count": 0,
          "type": "people",
          "url": "/people/ac113c7837148e5d43c18a06cf248d21",
          "url_token": "liuxiaoran-34",
          "user_type": "people",
          "vip_info": {
            "is_vip": false
          }
        },
        "can_top": false,
        "comment_count": 1,
        "comment_permission": "all",
        "comments": [],
        "content": [
          {
            "content": "长文LLM综述：长上下文大语言模型如是说<br>向大家宣传一下我们小组最近的长文综述<br>长上下文大语言模型如是说 Thus Spake Long-Context Large Language Model<br>arXiv: <a href=\"https://arxiv.org/abs/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://arxiv.org/abs/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://arxiv.org/abs/2502.17129</a><br>bilibili: <a href=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-icon-name=\"zhicon_icon_24_link\">https://www.bilibili.com/video/BV11h9AYoEYj</a> （划重点，欢迎大家一键三连 关注转发<br>hf: <a href=\"https://huggingface.co/papers/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://huggingface.co/papers/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://huggingface.co/papers/2502.17129</a><br>Github: <a href=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-icon-name=\"zhicon_icon_24_link\">https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM</a> （包括论文pdf和中文报告pdf，后续有英文pdf以及更详细版本更新 <br>小红书: <a href=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-icon-name=\"zhicon_icon_24_link\">http://xhslink.com/a/b7hwjYK2qsH6</a><br><br>这是一次将科研、音乐、哲学相结合起来的尝试！ <br>受理查·施特劳斯的交响诗《查拉图斯特拉如是说》的启发， <br>我们将当前大语言模型（LLM）扩长自身上下文长度的历程，<br>类比为人试图超越自身局限性的努力！<br>人通过激情、科学等的方法去追求永恒，<br>LLM通过架构、框架、训练等的优化去追求无限上下文！<br><br>在这篇综述中，我们从架构、框架、训练、评测四个方面出发， <br>涵盖长度外推、高效缓存、记忆管理、新架构、<br>训练框架、推理框架、长文预训练、长文后训练、<br>多模态长文（长视频）、长文评测共十个主题，<br>展示了长文LLM涉及到的各种技术，并且在文章末尾，<br>我们也列举了长文LLM仍然面临的一些问题。 <br><br>而在对应的视频中，伴随着交响曲的演进，<br>一篇篇长文的工作将在你眼前呈现，<br>长文研究的脉络将在你的眼前展开。<br>我们希望这样的呈现形式，能够<br>让不了解相关研究的朋友们感受到长文研究的架构脉络，<br>让了解长文工作的同行们感受到看到/看懂这些工作时候的感动。 <br><br>当然这是一种很创新的呈现形式，欢迎大家的关注！<br>如有不满，还请谅解，不喜勿喷，如有错误，还请指正。<br>我们会认真考虑这些建议，并在三个月后发布修订版。 <br>最后，特别感谢这篇文章合作者们的辛苦付出，<br>感谢华为诺亚实验室对小组科研工作的项目支持。<br><br><a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27460321\" data-pin-topic=\"zhihu://topic/27460321/pin20\">#论文综述</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/23668572\" data-pin-topic=\"zhihu://topic/23668572/pin20\">#科研生活</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28568717\" data-pin-topic=\"zhihu://topic/28568717/pin20\">#人工智能​</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27267395\" data-pin-topic=\"zhihu://topic/27267395/pin20\">#大语言模型</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19560026\" data-pin-topic=\"zhihu://topic/19560026/pin20\">#自然语言处理</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/945281748\" data-pin-topic=\"zhihu://topic/945281748/pin20\">#长上下文</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19551864\" data-pin-topic=\"zhihu://topic/19551864/pin20\">#古典音乐</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27504414\" data-pin-topic=\"zhihu://topic/27504414/pin20\">#查拉图斯特拉如是说</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19767214\" data-pin-topic=\"zhihu://topic/19767214/pin20\">#复旦</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28021839\" data-pin-topic=\"zhihu://topic/28021839/pin20\">#上海人工智能实验室</a>  ",
            "fold_type": "raw",
            "own_text": "长文LLM综述：长上下文大语言模型如是说<br>向大家宣传一下我们小组最近的长文综述<br>长上下文大语言模型如是说 Thus Spake Long-Context Large Language Model<br>arXiv: <a href=\"https://arxiv.org/abs/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://arxiv.org/abs/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://arxiv.org/abs/2502.17129</a><br>bilibili: <a href=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-icon-name=\"zhicon_icon_24_link\">https://www.bilibili.com/video/BV11h9AYoEYj</a> （划重点，欢迎大家一键三连 关注转发<br>hf: <a href=\"https://huggingface.co/papers/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://huggingface.co/papers/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://huggingface.co/papers/2502.17129</a><br>Github: <a href=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-icon-name=\"zhicon_icon_24_link\">https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM</a> （包括论文pdf和中文报告pdf，后续有英文pdf以及更详细版本更新 <br>小红书: <a href=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-icon-name=\"zhicon_icon_24_link\">http://xhslink.com/a/b7hwjYK2qsH6</a><br><br>这是一次将科研、音乐、哲学相结合起来的尝试！ <br>受理查·施特劳斯的交响诗《查拉图斯特拉如是说》的启发， <br>我们将当前大语言模型（LLM）扩长自身上下文长度的历程，<br>类比为人试图超越自身局限性的努力！<br>人通过激情、科学等的方法去追求永恒，<br>LLM通过架构、框架、训练等的优化去追求无限上下文！<br><br>在这篇综述中，我们从架构、框架、训练、评测四个方面出发， <br>涵盖长度外推、高效缓存、记忆管理、新架构、<br>训练框架、推理框架、长文预训练、长文后训练、<br>多模态长文（长视频）、长文评测共十个主题，<br>展示了长文LLM涉及到的各种技术，并且在文章末尾，<br>我们也列举了长文LLM仍然面临的一些问题。 <br><br>而在对应的视频中，伴随着交响曲的演进，<br>一篇篇长文的工作将在你眼前呈现，<br>长文研究的脉络将在你的眼前展开。<br>我们希望这样的呈现形式，能够<br>让不了解相关研究的朋友们感受到长文研究的架构脉络，<br>让了解长文工作的同行们感受到看到/看懂这些工作时候的感动。 <br><br>当然这是一种很创新的呈现形式，欢迎大家的关注！<br>如有不满，还请谅解，不喜勿喷，如有错误，还请指正。<br>我们会认真考虑这些建议，并在三个月后发布修订版。 <br>最后，特别感谢这篇文章合作者们的辛苦付出，<br>感谢华为诺亚实验室对小组科研工作的项目支持。<br><br><a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27460321\" data-pin-topic=\"zhihu://topic/27460321/pin20\">#论文综述</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/23668572\" data-pin-topic=\"zhihu://topic/23668572/pin20\">#科研生活</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28568717\" data-pin-topic=\"zhihu://topic/28568717/pin20\">#人工智能​</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27267395\" data-pin-topic=\"zhihu://topic/27267395/pin20\">#大语言模型</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19560026\" data-pin-topic=\"zhihu://topic/19560026/pin20\">#自然语言处理</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/945281748\" data-pin-topic=\"zhihu://topic/945281748/pin20\">#长上下文</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19551864\" data-pin-topic=\"zhihu://topic/19551864/pin20\">#古典音乐</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27504414\" data-pin-topic=\"zhihu://topic/27504414/pin20\">#查拉图斯特拉如是说</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19767214\" data-pin-topic=\"zhihu://topic/19767214/pin20\">#复旦</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28021839\" data-pin-topic=\"zhihu://topic/28021839/pin20\">#上海人工智能实验室</a>",
            "text_link_type": "internal",
            "title": "",
            "type": "text"
          }
        ],
        "content_html": "<div>长文LLM综述：长上下文大语言模型如是说<br/>向大家宣传一下我们小组最近的长文综述<br/>长上下文大语言模型如是说 Thus Spake Long-Context Large Language Model<br/>arXiv: <a href=\"https://arxiv.org/abs/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://arxiv.org/abs/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://arxiv.org/abs/2502.17129</a><br/>bilibili: <a href=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-icon-name=\"zhicon_icon_24_link\">https://www.bilibili.com/video/BV11h9AYoEYj</a> （划重点，欢迎大家一键三连 关注转发<br/>hf: <a href=\"https://huggingface.co/papers/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://huggingface.co/papers/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://huggingface.co/papers/2502.17129</a><br/>Github: <a href=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-icon-name=\"zhicon_icon_24_link\">https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM</a> （包括论文pdf和中文报告pdf，后续有英文pdf以及更详细版本更新 <br/>小红书: <a href=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-icon-name=\"zhicon_icon_24_link\">http://xhslink.com/a/b7hwjYK2qsH6</a><br/><br/>这是一次将科研、音乐、哲学相结合起来的尝试！ <br/>受理查·施特劳斯的交响诗《查拉图斯特拉如是说》的启发， <br/>我们将当前大语言模型（LLM）扩长自身上下文长度的历程，<br/>类比为人试图超越自身局限性的努力！<br/>人通过激情、科学等的方法去追求永恒，<br/>LLM通过架构、框架、训练等的优化去追求无限上下文！<br/><br/>在这篇综述中，我们从架构、框架、训练、评测四个方面出发， <br/>涵盖长度外推、高效缓存、记忆管理、新架构、<br/>训练框架、推理框架、长文预训练、长文后训练、<br/>多模态长文（长视频）、长文评测共十个主题，<br/>展示了长文LLM涉及到的各种技术，并且在文章末尾，<br/>我们也列举了长文LLM仍然面临的一些问题。 <br/><br/>而在对应的视频中，伴随着交响曲的演进，<br/>一篇篇长文的工作将在你眼前呈现，<br/>长文研究的脉络将在你的眼前展开。<br/>我们希望这样的呈现形式，能够<br/>让不了解相关研究的朋友们感受到长文研究的架构脉络，<br/>让了解长文工作的同行们感受到看到/看懂这些工作时候的感动。 <br/><br/>当然这是一种很创新的呈现形式，欢迎大家的关注！<br/>如有不满，还请谅解，不喜勿喷，如有错误，还请指正。<br/>我们会认真考虑这些建议，并在三个月后发布修订版。 <br/>最后，特别感谢这篇文章合作者们的辛苦付出，<br/>感谢华为诺亚实验室对小组科研工作的项目支持。<br/><br/><a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27460321\" data-pin-topic=\"zhihu://topic/27460321/pin20\">#论文综述</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/23668572\" data-pin-topic=\"zhihu://topic/23668572/pin20\">#科研生活</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28568717\" data-pin-topic=\"zhihu://topic/28568717/pin20\">#人工智能​</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27267395\" data-pin-topic=\"zhihu://topic/27267395/pin20\">#大语言模型</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19560026\" data-pin-topic=\"zhihu://topic/19560026/pin20\">#自然语言处理</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/945281748\" data-pin-topic=\"zhihu://topic/945281748/pin20\">#长上下文</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19551864\" data-pin-topic=\"zhihu://topic/19551864/pin20\">#古典音乐</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27504414\" data-pin-topic=\"zhihu://topic/27504414/pin20\">#查拉图斯特拉如是说</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19767214\" data-pin-topic=\"zhihu://topic/19767214/pin20\">#复旦</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28021839\" data-pin-topic=\"zhihu://topic/28021839/pin20\">#上海人工智能实验室</a>  </div>",
        "created": 1740674306,
        "creation_disclaimer": "",
        "excerpt_title": "长文LLM综述：长上下文大语言模型如是说<br>向大家宣传一下我们小组最近的长文综述<br>长上下文大语言模型如是说 Thus Spake Long-Context Large Language Model<br>arXiv: <a href=\"https://arxiv.org/abs/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://arxiv.org/abs/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://arxiv.org/abs/2502.17129</a><br>bilibili: <a href=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://www.bilibili.com/video/BV11h9AYoEYj\" data-icon-name=\"zhicon_icon_24_link\">https://www.bilibili.com/video/BV11h9AYoEYj</a> （划重点，欢迎大家一键三连 关注转发<br>hf: <a href=\"https://huggingface.co/papers/2502.17129\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://huggingface.co/papers/2502.17129\" data-icon-name=\"zhicon_icon_24_link\">https://huggingface.co/papers/2502.17129</a><br>Github: <a href=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM\" data-icon-name=\"zhicon_icon_24_link\">https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM</a> （包括论文pdf和中文报告pdf，后续有英文pdf以及更详细版本更新 <br>小红书: <a href=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-insert-way=\"force\" data-draft-node=\"inline\" data-draft-type=\"text-link\" class=\"internal\" data-icon-type=\"link\" data-original-url=\"http://xhslink.com/a/b7hwjYK2qsH6\" data-icon-name=\"zhicon_icon_24_link\">http://xhslink.com/a/b7hwjYK2qsH6</a><br><br>这是一次将科研、音乐、哲学相结合起来的尝试！ <br>受理查·施特劳斯的交响诗《查拉图斯特拉如是说》的启发， <br>我们将当前大语言模型（LLM）扩长自身上下文长度的历程，<br>类比为人试图超越自身局限性的努力！<br>人通过激情、科学等的方法去追求永恒，<br>LLM通过架构、框架、训练等的优化去追求无限上下文！<br><br>在这篇综述中，我们从架构、框架、训练、评测四个方面出发， <br>涵盖长度外推、高效缓存、记忆管理、新架构、<br>训练框架、推理框架、长文预训练、长文后训练、<br>多模态长文（长视频）、长文评测共十个主题，<br>展示了长文LLM涉及到的各种技术，并且在文章末尾，<br>我们也列举了长文LLM仍然面临的一些问题。 <br><br>而在对应的视频中，伴随着交响曲的演进，<br>一篇篇长文的工作将在你眼前呈现，<br>长文研究的脉络将在你的眼前展开。<br>我们希望这样的呈现形式，能够<br>让不了解相关研究的朋友们感受到长文研究的架构脉络，<br>让了解长文工作的同行们感受到看到/看懂这些工作时候的感动。 <br><br>当然这是一种很创新的呈现形式，欢迎大家的关注！<br>如有不满，还请谅解，不喜勿喷，如有错误，还请指正。<br>我们会认真考虑这些建议，并在三个月后发布修订版。 <br>最后，特别感谢这篇文章合作者们的辛苦付出，<br>感谢华为诺亚实验室对小组科研工作的项目支持。<br><br><a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27460321\" data-pin-topic=\"zhihu://topic/27460321/pin20\">#论文综述</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/23668572\" data-pin-topic=\"zhihu://topic/23668572/pin20\">#科研生活</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28568717\" data-pin-topic=\"zhihu://topic/28568717/pin20\">#人工智能​</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27267395\" data-pin-topic=\"zhihu://topic/27267395/pin20\">#大语言模型</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19560026\" data-pin-topic=\"zhihu://topic/19560026/pin20\">#自然语言处理</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/945281748\" data-pin-topic=\"zhihu://topic/945281748/pin20\">#长上下文</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19551864\" data-pin-topic=\"zhihu://topic/19551864/pin20\">#古典音乐</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27504414\" data-pin-topic=\"zhihu://topic/27504414/pin20\">#查拉图斯特拉如是说</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/19767214\" data-pin-topic=\"zhihu://topic/19767214/pin20\">#复旦</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28021839\" data-pin-topic=\"zhihu://topic/28021839/pin20\">#上海人工智能实验室</a>  ",
        "favlists_count": 1,
        "favorite_count": 1,
        "id": "1878604872380814766",
        "is_admin_close_repin": false,
        "is_contain_ai_content": false,
        "is_deleted": false,
        "is_top": false,
        "like_count": 2,
        "likers": [],
        "long_threshold": [
          {
            "height": 3,
            "width": 1
          },
          {
            "height": 1,
            "width": 3
          }
        ],
        "meet_reaction_guide_conditions": false,
        "page_view_count": null,
        "questions": null,
        "reaction": {
          "image_reactions": {},
          "relation": {
            "current_user_is_navigator": false,
            "faved": true,
            "following": false,
            "is_author": false,
            "is_navigator_vote": false,
            "liked": false,
            "subcribed": false,
            "vote": "Neutral",
            "vote_next_step": ""
          },
          "statistics": {
            "applaud_count": 2,
            "bullet_count": 0,
            "comment_count": 1,
            "down_vote_count": 0,
            "favorites": 1,
            "interest_play_count": 0,
            "like_count": 0,
            "plaincontent_like_count": 0,
            "plaincontent_vote_up_count": 0,
            "play_count": 0,
            "pv_count": 0,
            "question_answer_count": 0,
            "question_follower_count": 0,
            "republishers": [],
            "share_count": 0,
            "subscribe_count": 0,
            "up_vote_count": 2
          }
        },
        "reaction_count": 2,
        "reaction_instruction": {},
        "reaction_relation": {
          "like": 0,
          "vote": 2
        },
        "regulate_info": {
          "is_regulating": false
        },
        "repin_count": 0,
        "ring_info": null,
        "self_create": false,
        "source_pin_id": 0,
        "state": "normal",
        "tag_specials": {},
        "tags": [
          "#论文综述</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/23668572\" data-pin-topic=\"zhihu://topic/23668572/pin20\">#",
          "#人工智能​</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27267395\" data-pin-topic=\"zhihu://topic/27267395/pin20\">#",
          "#自然语言处理</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/945281748\" data-pin-topic=\"zhihu://topic/945281748/pin20\">#",
          "#古典音乐</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/27504414\" data-pin-topic=\"zhihu://topic/27504414/pin20\">#",
          "#复旦</a>  <a class=\"hash_tag\" href=\"https://www.zhihu.com/topic/28021839\" data-pin-topic=\"zhihu://topic/28021839/pin20\">#"
        ],
        "top_reactions": {
          "like": 2
        },
        "topics": [
          {
            "id": "28021839",
            "name": "上海人工智能实验室",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/28021839/pin20"
          },
          {
            "id": "19767214",
            "name": "复旦",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/19767214/pin20"
          },
          {
            "id": "27504414",
            "name": "查拉图斯特拉如是说",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/27504414/pin20"
          },
          {
            "id": "19551864",
            "name": "古典音乐",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/19551864/pin20"
          },
          {
            "id": "945281748",
            "name": "长上下文",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/945281748/pin20"
          },
          {
            "id": "19560026",
            "name": "自然语言处理",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/19560026/pin20"
          },
          {
            "id": "27267395",
            "name": "大语言模型",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/27267395/pin20"
          },
          {
            "id": "28568717",
            "name": "人工智能​",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/28568717/pin20"
          },
          {
            "id": "23668572",
            "name": "科研生活",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/23668572/pin20"
          },
          {
            "id": "27460321",
            "name": "论文综述",
            "topic_type": "NORMAL",
            "type": "topic",
            "url": "zhihu://topic/27460321/pin20"
          }
        ],
        "type": "pin",
        "updated": 1740674306,
        "url": "https://www.zhihu.com/pin/1878604872380814766?native=0",
        "view_permission": "all",
        "virtuals": {
          "is_favorited": true,
          "is_liked": false
        }
      },
      "verb": "MEMBER_CREATE_PIN",
      "created_time": 1740674306,
      "interaction": {
        "can_show_sticker": true
      },
      "actor": {
        "id": "ac113c7837148e5d43c18a06cf248d21",
        "name": "河畔草lxr",
        "headline": "",
        "type": "people",
        "user_type": "people",
        "url": "https://www.zhihu.com/people/liuxiaoran-34",
        "url_token": "liuxiaoran-34",
        "avatar_url": "https://pica.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_l.jpg?source=5a24d060&needBackground=1",
        "gender": 1,
        "is_following": false,
        "is_followed": false,
        "is_org": false,
        "badge": [],
        "badge_v2": {
          "title": "",
          "merged_badges": [],
          "detail_badges": [],
          "icon": "",
          "night_icon": ""
        },
        "vip_info": {
          "is_vip": false,
          "target_url": ""
        },
        "kvip_info": {
          "is_vip": false,
          "target_url": ""
        }
      },
      "action_text": "发布了想法",
      "is_sticky": false
    },
    {
      "id": "1733028770059",
      "type": "feed",
      "target": {
        "id": "9300089039",
        "type": "article",
        "author": {
          "id": "f16a06041ffac91a1c0d31da4658f5a8",
          "name": "sonta",
          "headline": "PhD student @ MIT CSAIL",
          "type": "people",
          "user_type": "people",
          "url": "https://www.zhihu.com/people/buhezuobugaoxing",
          "url_token": "buhezuobugaoxing",
          "avatar_url": "https://picx.zhimg.com/v2-b1712309c6b65f0a7b292122b4dd82f9_l.jpg?source=5a24d060&needBackground=1",
          "gender": 0,
          "is_following": false,
          "is_followed": false,
          "is_org": false,
          "badge": [],
          "badge_v2": {
            "title": "",
            "merged_badges": [],
            "detail_badges": [],
            "icon": "",
            "night_icon": ""
          },
          "vip_info": {
            "is_vip": true,
            "vip_icon": {
              "url": "https://picx.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a_r.jpg?source=5a24d060",
              "night_mode_url": "https://picx.zhimg.com/v2-c9686ff064ea3579730756ac6c289978_r.jpg?source=5a24d060"
            },
            "target_url": ""
          },
          "kvip_info": {
            "is_vip": false,
            "target_url": ""
          }
        },
        "created": 1732685105,
        "updated": 1732768920,
        "title": "在ICLR openreview主持正义，锐评Were RNNs all we needed?",
        "excerpt_title": "",
        "content": "<p data-pid=\"PFgDf3uy\"><i>TL;DR:  实在受不了傻13工作还有眼瞎reviewer给8分，并且作者在rebuttal阶段全程嘴硬，遂直接在Openreview贴脸开大，正义制裁： </i><a href=\"https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3DGrmFFxGnOR%26noteId%3D2QR0ZJjvCm\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">openreview.net/forum?</span><span class=\"invisible\">id=GrmFFxGnOR&amp;noteId=2QR0ZJjvCm</span><span class=\"ellipsis\"></span></a></p><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-fc08e25c8bc2d09c9382666e23411abb_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1128\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1128\" data-original=\"https://pic2.zhimg.com/v2-fc08e25c8bc2d09c9382666e23411abb_r.jpg\" data-original-token=\"v2-fc08e25c8bc2d09c9382666e23411abb\"/><figcaption>如果ICLR变成了一个bad会议，那么你就去建设它 ——sonta</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p data-pid=\"G67zPVs8\"><i>Were RNNs all we needed? 受到了毫无相关技术背景的小编们的无脑吹捧和热议</i></p><p data-pid=\"aVn5vPRt\"><a href=\"https://zhuanlan.zhihu.com/p/1047342846\" class=\"internal\" target=\"_blank\">机器之心：图灵奖得主Yoshua Bengio新作：Were RNNs All We Needed?</a></p><p data-pid=\"rRkKJFpc\"><a href=\"https://zhuanlan.zhihu.com/p/3490927331\" class=\"internal\" target=\"_blank\">量子位：Bengio精简了传统RNN，性能可与Transformer媲美</a></p><p data-pid=\"bP_RsfOq\"><a href=\"https://zhuanlan.zhihu.com/p/3126755629\" class=\"internal\" target=\"_blank\">新智元：RNN回归！Bengio新作大道至简与Transformer一较高下</a></p><p data-pid=\"_U4slQji\">这篇文章核心思路就是把hidden to hidden state之间的nonlinear dependency全部扔掉，然后RNN就变成了一个Linear RNN，可以用associative scan的思路来并行训练</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-5ae673abd99362be6251d0e53b8bf0db_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1448\" data-rawheight=\"620\" class=\"origin_image zh-lightbox-thumb\" width=\"1448\" data-original=\"https://picx.zhimg.com/v2-5ae673abd99362be6251d0e53b8bf0db_r.jpg\" data-original-token=\"v2-5ae673abd99362be6251d0e53b8bf0db\"/></figure><p data-pid=\"QJ2M2uUa\">然而这个idea早就在linear RNN领域里玩烂了。其中ICLR &#39;18最经典也最被低估的一篇工作 <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1709.04057\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Parallelizing Linear Recurrent Neural Nets Over Sequence Length</a> 里提到的Gated Impluse Linear Recurrent (GILR) layer几乎完全等价于minGRU.</p><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-278e1bd7dd783ef0af81d74736bdf186_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1808\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb\" width=\"1808\" data-original=\"https://pica.zhimg.com/v2-278e1bd7dd783ef0af81d74736bdf186_r.jpg\" data-original-token=\"v2-278e1bd7dd783ef0af81d74736bdf186\"/></figure><p data-pid=\"87lsOPh9\">我在之前也早有blog来理清这一系列的工作的发展</p><a href=\"https://zhuanlan.zhihu.com/p/616357772\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/equation.jpg\" data-image-width=\"0\" data-image-height=\"0\" class=\"internal\">sonta：RNN最简单有效的形式是什么？</a><p data-pid=\"b78fs3Dm\">并且我NeurIPS &#39;23 Spotlight的一个工作，HGRN，正是基于这个思路来进行的</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2311.04823\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://picx.zhimg.com/v2-4baaae2386ede0213c693947a141a747_qhd.jpg\" data-image-width=\"1200\" data-image-height=\"700\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Hierarchically Gated Recurrent Neural Network for Sequence Modeling</a><p data-pid=\"K2XI6xIf\">其中有一个审稿人锐评道：</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-720a23c561660759a973d25023f5c2c1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://picx.zhimg.com/v2-720a23c561660759a973d25023f5c2c1_r.jpg\" data-original-token=\"v2-720a23c561660759a973d25023f5c2c1\"/><figcaption>https://openreview.net/forum?id=GrmFFxGnOR&amp;noteId=ZLKbCUEU7B 同时也是所有审稿人里唯一很懂领域的人</figcaption></figure><p data-pid=\"Mi5Zf2z9\">作者的狡辩是：</p><blockquote data-pid=\"jJEzH2iE\">Martin &amp; Cundy (2018) focus on parallelizing linear RNNs and propose the GILR (Generalized Linear RNN) architecture. GILR is used as a linear surrogate for the hidden state dependencies of traditional LSTMs, allowing for parallelization. The resulting architecture GILR-LSTM retains much of the complexity of LSTMs but with parallelizability, resulting in a larger memory footprint due to the use of surrogate states.</blockquote><p data-pid=\"5PwiFrQP\">这是十足的偷换概念： 我们可以在上面的recurrent形式可以看到，GILR完全等价于minGRU。作者在用另外一个extension GILR-LSTM来混淆视听，狡辩道</p><blockquote data-pid=\"BSEBtUUv\"> Martin &amp; Cundy (2018) focus on parallelizing linear RNNs and propose the GILR (Generalized Linear RNN) architecture. GILR is used as a linear surrogate for the hidden state dependencies of traditional LSTMs, allowing for parallelization. The resulting architecture GILR-LSTM retains much of the complexity of LSTMs but with parallelizability, resulting in a larger memory footprint due to the use of surrogate states. <b>In contrast, our work takes a different approach by simplifying traditional RNN architectures rather than augmenting them</b></blockquote><p data-pid=\"lYwITEEs\">这种被审稿人按着捶还不认错，反过来混淆视听的做法真是一点b脸都不要了。此外，这篇文章的完成度低的令人发指，几乎没有稍微大一点的实验。对此，某图灵奖得主挂名的团队给的解释是：</p><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-643cd4295f92a681bec4138d8ab02daf_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1906\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb\" width=\"1906\" data-original=\"https://pic2.zhimg.com/v2-643cd4295f92a681bec4138d8ab02daf_r.jpg\" data-original-token=\"v2-643cd4295f92a681bec4138d8ab02daf\"/><figcaption>我们穷的只能用P100辣。然而据我mila朋友的反应，mila内部有不少计算集群资源，并且这个一作的权限还不低。</figcaption></figure><p data-pid=\"5Pc5mdpE\">而我们发表在一年前的HGRN就已经做了Billion level的language modeling的实验了。看不下去的我直接openreview发了个public comment:</p><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-1a21855b9ed50ada005ca2ffcde18e74_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"888\" class=\"origin_image zh-lightbox-thumb\" width=\"1592\" data-original=\"https://pica.zhimg.com/v2-1a21855b9ed50ada005ca2ffcde18e74_r.jpg\" data-original-token=\"v2-1a21855b9ed50ada005ca2ffcde18e74\"/></figure><p data-pid=\"Kqqb3Ffq\">提到了我们HGRN和之后一系列的work，并且重新提了minGRU和GILR的关系。作者依然用上面糊弄审稿人的做法来糊弄我，</p><p data-pid=\"9k0amjhU\">并且解释道这篇的motivation是为了simplify existing architecture</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-b4ddadcd684d19a81061ff39fc8746e7_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1432\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"1432\" data-original=\"https://picx.zhimg.com/v2-b4ddadcd684d19a81061ff39fc8746e7_r.jpg\" data-original-token=\"v2-b4ddadcd684d19a81061ff39fc8746e7\"/></figure><p data-pid=\"pR7Tonjx\">变得新手友好，老少皆宜。</p><p data-pid=\"iU_BPc5D\">我哭笑不得，首先勒令他们解释跟GILR的区别，</p><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-37da472e03bb9fd2131af3dacabb43fa_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https://pica.zhimg.com/v2-37da472e03bb9fd2131af3dacabb43fa_r.jpg\" data-original-token=\"v2-37da472e03bb9fd2131af3dacabb43fa\"/><figcaption>你说的好，但是GILR=minGRU</figcaption></figure><p data-pid=\"Hu2zsP17\">并且痛批他们开倒车，把整个领域这一年的发展直接清零，梦回一年半前</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-7f842524f1bcd98cd3e15c54c3d60c8d_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb\" width=\"966\" data-original=\"https://pic4.zhimg.com/v2-7f842524f1bcd98cd3e15c54c3d60c8d_r.jpg\" data-original-token=\"v2-7f842524f1bcd98cd3e15c54c3d60c8d\"/><figcaption>大家辛辛苦苦发展了一年的领域，你直接倒回去了？</figcaption></figure><p data-pid=\"6v_O4rHJ\">同时作者团队还在解释，不是他们博人眼球特意宣传，是大家自发进行的：</p><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-479eb8a9c29b33ecc95ca9a3eee51a42_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1332\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1332\" data-original=\"https://pica.zhimg.com/v2-479eb8a9c29b33ecc95ca9a3eee51a42_r.jpg\" data-original-token=\"v2-479eb8a9c29b33ecc95ca9a3eee51a42\"/></figure><p data-pid=\"IX-p8OxB\">（内心OS：对对对，你们有这么大一个图灵奖得主挂名，标题起的这么大，可不得是一堆自干五来做宣传吗）</p><p data-pid=\"igTfcMau\">我的使命到此结束，等待AC和其他审稿人的正义制裁</p><h2>结言</h2><p data-pid=\"hmALENaH\">肉眼可见，ICLR ‘25 是一届特别糟糕的会议。主办方大聪明觉得把审稿人的池子放水，引入了一堆本来没有资格审稿的人加入审稿大军，那么每个审稿人的workload不就小了吗？这一大放水的结果可想而知，这么离谱的一篇工作，还有两个reviewer给出了8分和6分的高分，并且8分reviewer直呼novel，气的我反手一个public comment:</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-06cb1336e534af452bb2cee99b69fd5e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"926\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb\" width=\"926\" data-original=\"https://pic3.zhimg.com/v2-06cb1336e534af452bb2cee99b69fd5e_r.jpg\" data-original-token=\"v2-06cb1336e534af452bb2cee99b69fd5e\"/></figure><p data-pid=\"--tZjMG4\">审稿workload变低了，但审稿质量离谱了，大家又得花更多的时间去处理不称职审稿人的意见，导致本届会议的discussion氛围感人，主办方直接延期rebuttal一周，让大家感受三周rebuttal的快乐，真是不戳呢（</p><p data-pid=\"ljfMjtqD\">同时，我发现审稿人对linear RNN的进展的了解真是少的令人发指，我一个一年前的starting point工作，被人重新包装了一遍还能受到一些好评。感觉提高自己工作的曝光度是一件非常重要的事情，教育community也是（不多说了，赶紧去写blog来系统的介绍自己这两年的工作了）</p>",
        "excerpt": "<i>TL;DR: 实在受不了傻13工作还有眼瞎reviewer给8分，并且作者在rebuttal阶段全程嘴硬，遂直接在Openreview贴脸开大，正义制裁： </i><a href=\"https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3DGrmFFxGnOR%26noteId%3D2QR0ZJjvCm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://http://openreview.net/forum?id=GrmFFxGnOR&amp;noteId=2QR0ZJjvCm</a>如果ICLR变成了一个bad会议，那么你就去建设它 ——sonta <i>Were RNNs all we needed? 受到了毫无相关技术背景的小编们的无脑吹捧和热议</i> <a href=\"https://zhuanlan.zhihu.com/p/1047342846\" class=\"internal\" target=\"_blank\">机器之心：图灵奖得主Yoshua Bengio新作：Were RNNs All We Needed?</a> <a href=\"https://zhuanlan.zhihu.com/p/3490927331\" class=\"internal\" target=\"_blank\">量子位：Bengio精简了传…</a>",
        "excerpt_new": "<i>TL;DR: 实在受不了傻13工作还有眼瞎reviewer给8分，并且作者在rebuttal阶段全程嘴硬，遂直接在Openreview贴脸开大，正义制裁： </i><a href=\"https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3DGrmFFxGnOR%26noteId%3D2QR0ZJjvCm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://http://openreview.net/forum?id=GrmFFxGnOR&amp;noteId=2QR0ZJjvCm</a>如果ICLR变成了一个bad会议，那么你就去建设它 ——sonta <i>Were RNNs all we needed? 受到了毫无相关技术背景的小编们的无脑吹捧和热议</i> <a href=\"https://zhuanlan.zhihu.com/p/1047342846\" class=\"internal\" target=\"_blank\">机器之心：图灵奖得主Yoshua Bengio新作：Were RNNs All We Needed?</a> <a href=\"https://zhuanlan.zhihu.com/p/3490927331\" class=\"internal\" target=\"_blank\">量子位：Bengio精简了传…</a>",
        "preview_type": "default",
        "preview_text": "",
        "url": "https://zhuanlan.zhihu.com/p/9300089039",
        "comment_permission": "all",
        "voteup_count": 1781,
        "comment_count": 132,
        "image_url": "https://picx.zhimg.com/v2-8ffa6b68840f40ce179a4d908c7ea326_r.jpg?source=172ae18b",
        "linkbox": {
          "url": "",
          "category": "",
          "pic": "",
          "title": ""
        },
        "reaction_instruction": {},
        "upvoted_followees": [],
        "voting": 0,
        "content_need_truncated": false,
        "force_login_when_click_read_more": false,
        "is_navigator": false,
        "navigator_vote": false,
        "vote_next_step": "vote"
      },
      "verb": "MEMBER_VOTEUP_ARTICLE",
      "created_time": 1733028770,
      "interaction": {
        "can_show_sticker": true
      },
      "actor": {
        "id": "ac113c7837148e5d43c18a06cf248d21",
        "name": "河畔草lxr",
        "headline": "",
        "type": "people",
        "user_type": "people",
        "url": "https://www.zhihu.com/people/liuxiaoran-34",
        "url_token": "liuxiaoran-34",
        "avatar_url": "https://pic1.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_l.jpg?source=5a24d060&needBackground=1",
        "gender": 1,
        "is_following": false,
        "is_followed": false,
        "is_org": false,
        "badge": [],
        "badge_v2": {
          "title": "",
          "merged_badges": [],
          "detail_badges": [],
          "icon": "",
          "night_icon": ""
        },
        "vip_info": {
          "is_vip": false,
          "target_url": ""
        },
        "kvip_info": {
          "is_vip": false,
          "target_url": ""
        }
      },
      "action_text": "赞同了文章",
      "is_sticky": false
    },
    {
      "id": "1720149082212",
      "type": "feed",
      "target": {
        "id": "707065312",
        "type": "article",
        "author": {
          "id": "bfcfd15c45e6c4a3e1cfd94e032faa54",
          "name": "书生大模型",
          "headline": "https://chat.intern-ai.org.cn/",
          "type": "people",
          "user_type": "people",
          "url": "https://www.zhihu.com/people/InternLM",
          "url_token": "InternLM",
          "avatar_url": "https://picx.zhimg.com/v2-c132c17c7f65728bf8fb57e4a7493fd3_l.jpg?source=5a24d060&needBackground=1",
          "gender": 1,
          "is_following": false,
          "is_followed": false,
          "is_org": false,
          "badge": [],
          "badge_v2": {
            "title": "",
            "merged_badges": [],
            "detail_badges": [],
            "icon": "",
            "night_icon": ""
          },
          "vip_info": {
            "is_vip": false,
            "target_url": ""
          },
          "kvip_info": {
            "is_vip": false,
            "target_url": ""
          }
        },
        "created": 1720087839,
        "updated": 1720680637,
        "title": "书生·浦语2.5开源，推理能力再创新标杆",
        "excerpt_title": "",
        "content": "<p data-pid=\"TE1uLSde\">2024 年 7 月 3 日，上海人工智能实验室与商汤科技联合香港中文大学和复旦大学正式发布新一代大语言模型书⽣·浦语2.5（InternLM2.5）。相比上一代模型，InternLM2.5 有三项突出亮点：</p><ul><li data-pid=\"vQ1tQxdX\">推理能力大幅提升，领先于国内外同量级开源模型，在部分维度上甚至超越十倍量级的 Llama3-70B；</li><li data-pid=\"I8Yi7Tpp\">支持 1M tokens 上下文，能够处理百万字长文；</li><li data-pid=\"B-ddcODg\">具有强大的自主规划和工具调用能力，比如可以针对复杂问题，搜索上百个网页并进行整合分析。</li></ul><p data-pid=\"GoGE7OLr\">InternLM2.5-7B 模型即日起开源可用，更大和更小的模型也将在近期发布开源。上海人工智能实验室秉持“以持续的高质量开源赋能创新”理念，在为社区始终如一地提供高质量开源模型的同时，也将继续坚持免费商用授权。</p><p data-pid=\"GAx40t4s\"><br/>GitHub 链接：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/InternLM/InternLM\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/InternLM/Int</span><span class=\"invisible\">ernLM</span><span class=\"ellipsis\"></span></a><br/>HuggingFace 模型：<a href=\"https://link.zhihu.com/?target=https%3A//huggingface.co/internlm\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">huggingface.co/internlm</span><span class=\"invisible\"></span></a><br/>书生·浦语主页：<a href=\"https://link.zhihu.com/?target=https%3A//internlm.intern-ai.org.cn/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">internlm.intern-ai.org.cn</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a></p><h2>性能加速器：合成数据+模型飞轮</h2><p data-pid=\"wP1u43av\"> 随着大模型的快速发展，人类积累的数据也在快速消耗，如何高效地提升模型性能成为了当前面临的重大挑战。为此，我们研发了新的合成数据和模型飞轮，一方面通过合成数据弥补领域高质量数据的不足，另一方面通过模型的自我迭代不断完成数据提升和缺陷修复，从而大大加快了 InternLM2.5 的迭代。</p><p data-pid=\"DL4GuL0R\">针对不同的数据特点，我们制定了多种的数据合成技术方案，保障不同类型合成数据的质量，包括基于规则的数据构造，基于模型的数据扩充，和基于反馈的数据生成。</p><p data-pid=\"ywSr8QIQ\">在研发过程中，模型本身也被持续不断地应用于模型的迭代。我们基于当前模型构建了多智能体用于数据的筛选、评估和标注，大幅提升了数据质量和多样性。同时也使用模型进行新语料的生产和精炼，让模型能够随着训练过程对发现的问题进行修复。</p><h2>领先的推理能力</h2><p data-pid=\"C3lR5ZyH\">强大的推理能力是大模型通向通用人工智能的重要基础，InternLM2.5 将推理能力作为模型最核心的能力进行优化，为复杂场景的应用落地提供了良好的基础。</p><p data-pid=\"BDEJmsfF\">基于司南 OpenCompass 开源评测框架，研究团队使用统一可复现的评测方法在多个推理能力权威评测集上进行了评测。相比上一代模型，InternLM2.5 在多个推理能力权威评测集上实现了大幅性能提升，尤其在由竞赛问题构成的数学评测集 MATH 上更是提升100%，以 7B 参数达到了 60% 的准确率（达到 GPT-4 Turbo 1106 版本的水平），充分展示了模型在数学推理上的优异成绩。</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-17425ee5e6b7892622b9d0af2bc0bf63_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"534\" class=\"origin_image zh-lightbox-thumb\" width=\"682\" data-original=\"https://picx.zhimg.com/v2-17425ee5e6b7892622b9d0af2bc0bf63_r.jpg\" data-original-token=\"v2-17425ee5e6b7892622b9d0af2bc0bf63\"/></figure><p data-pid=\"lO22Ex2h\">在过去的几个月里，大模型开源社区快速发展，涌现了一大批优秀的模型。和国内外最新的开源模型相比，InternLM2.5-7B-Chat 在绝大部分推理评测集上领先于同量级模型，相比前代模型更是实现了跨量级的性能超越，在部分维度上甚至赶超了十倍体量的 Llama3-70B-Instruct。</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-06d0abbe9b255027b9506b4f41802d32_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1686\" data-rawheight=\"841\" class=\"origin_image zh-lightbox-thumb\" width=\"1686\" data-original=\"https://pic3.zhimg.com/v2-06d0abbe9b255027b9506b4f41802d32_r.jpg\" data-original-token=\"v2-06d0abbe9b255027b9506b4f41802d32\"/></figure><p data-pid=\"jgg3yPJJ\"><br/>我们通过一个例子来展示 InternLM2.5 优秀的推理能力。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">User</span><span class=\"p\">:</span>\n\n<span class=\"o\">|</span> <span class=\"err\">排名</span> <span class=\"o\">|</span> <span class=\"err\">国家</span> <span class=\"o\">/</span> <span class=\"err\">地区</span>          <span class=\"o\">|</span> <span class=\"err\">金牌</span> <span class=\"o\">|</span> <span class=\"err\">银牌</span> <span class=\"o\">|</span> <span class=\"err\">铜牌</span> <span class=\"o\">|</span> <span class=\"err\">总计</span> <span class=\"o\">|</span>\n<span class=\"o\">|------|---------------------|------|------|------|------|</span>\n<span class=\"o\">|</span> <span class=\"mi\">1</span>    <span class=\"o\">|</span> <span class=\"err\">美国（</span><span class=\"n\">USA</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">39</span>   <span class=\"o\">|</span> <span class=\"mi\">41</span>   <span class=\"o\">|</span> <span class=\"mi\">33</span>   <span class=\"o\">|</span> <span class=\"mi\">113</span>  <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">2</span>    <span class=\"o\">|</span> <span class=\"err\">中国（</span><span class=\"n\">CHN</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">38</span>   <span class=\"o\">|</span> <span class=\"mi\">32</span>   <span class=\"o\">|</span> <span class=\"mi\">19</span>   <span class=\"o\">|</span> <span class=\"mi\">89</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">3</span>    <span class=\"o\">|</span> <span class=\"err\">日本（</span><span class=\"n\">JPN</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">27</span>   <span class=\"o\">|</span> <span class=\"mi\">14</span>   <span class=\"o\">|</span> <span class=\"mi\">17</span>   <span class=\"o\">|</span> <span class=\"mi\">58</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">4</span>    <span class=\"o\">|</span> <span class=\"err\">英国（</span><span class=\"n\">GBR</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">22</span>   <span class=\"o\">|</span> <span class=\"mi\">20</span>   <span class=\"o\">|</span> <span class=\"mi\">22</span>   <span class=\"o\">|</span> <span class=\"mi\">64</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">5</span>    <span class=\"o\">|</span> <span class=\"err\">俄罗斯奥林匹克委员会（</span><span class=\"n\">ROC</span><span class=\"err\">）</span><span class=\"o\">|</span> <span class=\"mi\">20</span>   <span class=\"o\">|</span> <span class=\"mi\">28</span>   <span class=\"o\">|</span> <span class=\"mi\">23</span>   <span class=\"o\">|</span> <span class=\"mi\">71</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">6</span>    <span class=\"o\">|</span> <span class=\"err\">澳大利亚（</span><span class=\"n\">AUS</span><span class=\"err\">）</span>     <span class=\"o\">|</span> <span class=\"mi\">17</span>   <span class=\"o\">|</span> <span class=\"mi\">7</span>    <span class=\"o\">|</span> <span class=\"mi\">22</span>   <span class=\"o\">|</span> <span class=\"mi\">46</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">7</span>    <span class=\"o\">|</span> <span class=\"err\">荷兰（</span><span class=\"n\">NED</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">10</span>   <span class=\"o\">|</span> <span class=\"mi\">12</span>   <span class=\"o\">|</span> <span class=\"mi\">14</span>   <span class=\"o\">|</span> <span class=\"mi\">36</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">8</span>    <span class=\"o\">|</span> <span class=\"err\">法国（</span><span class=\"n\">FRA</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">10</span>   <span class=\"o\">|</span> <span class=\"mi\">12</span>   <span class=\"o\">|</span> <span class=\"mi\">11</span>   <span class=\"o\">|</span> <span class=\"mi\">33</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">9</span>    <span class=\"o\">|</span> <span class=\"err\">德国（</span><span class=\"n\">GER</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">10</span>   <span class=\"o\">|</span> <span class=\"mi\">11</span>   <span class=\"o\">|</span> <span class=\"mi\">16</span>   <span class=\"o\">|</span> <span class=\"mi\">37</span>   <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">10</span>   <span class=\"o\">|</span> <span class=\"err\">意大利（</span><span class=\"n\">ITA</span><span class=\"err\">）</span>       <span class=\"o\">|</span> <span class=\"mi\">10</span>   <span class=\"o\">|</span> <span class=\"mi\">10</span>   <span class=\"o\">|</span> <span class=\"mi\">20</span>   <span class=\"o\">|</span> <span class=\"mi\">40</span>   <span class=\"o\">|</span>\n\n<span class=\"err\">根据提供的</span><span class=\"mi\">2020</span><span class=\"err\">年东京奥运会奖牌榜部分信息，请回答列表中属于北美洲国家和大洋洲国家的银牌奖牌和是多少？</span>\n\n<span class=\"n\">Assistant</span><span class=\"p\">:</span> \n\n<span class=\"err\">根据提供的信息，北美洲和大洋洲国家的银牌奖牌数如下：</span>\n\n<span class=\"err\">北美洲国家：</span>\n<span class=\"o\">-</span> <span class=\"err\">美国（</span><span class=\"n\">USA</span><span class=\"err\">）：</span><span class=\"mi\">41</span><span class=\"err\">枚银牌</span>\n<span class=\"o\">-</span> <span class=\"err\">加拿大（未在列表中，但作为北美洲国家，其银牌数未提供）</span>\n\n<span class=\"err\">大洋洲国家：</span>\n<span class=\"o\">-</span> <span class=\"err\">澳大利亚（</span><span class=\"n\">AUS</span><span class=\"err\">）：</span><span class=\"mi\">7</span><span class=\"err\">枚银牌</span>\n\n<span class=\"err\">因此，北美洲和大洋洲国家的银牌总数为：</span>\n<span class=\"mi\">41</span><span class=\"err\">（美国）</span><span class=\"o\">+</span> <span class=\"mi\">7</span><span class=\"err\">（澳大利亚）</span><span class=\"o\">=</span> <span class=\"mi\">48</span><span class=\"err\">枚银牌。</span>\n\n<span class=\"n\">User</span><span class=\"p\">:</span>\n\n<span class=\"err\">根据我提供的信息，现在请你在排名前</span><span class=\"mi\">3</span><span class=\"err\">的国家中，将金、银、铜牌每一项都至少有</span><span class=\"mi\">15</span><span class=\"err\">枚的所有国家筛选出来，绘制成表格，请一步步思考来完成这个问题</span>\n\n<span class=\"n\">Assistant</span><span class=\"p\">:</span>\n\n<span class=\"err\">好的，让我们一步步来解决这个问题。</span>\n\n<span class=\"err\">首先，我们需要确定排名前</span><span class=\"mi\">3</span><span class=\"err\">的国家，根据您提供的信息，这些国家是美国、中国和日本。</span>\n\n<span class=\"err\">接下来，我们需要筛选出这些国家中，金、银、铜牌每一项都至少有</span><span class=\"mi\">15</span><span class=\"err\">枚的国家。</span>\n\n<span class=\"err\">让我们逐一检查这些国家：</span>\n\n<span class=\"mf\">1.</span> <span class=\"err\">美国（</span><span class=\"n\">USA</span><span class=\"err\">）：金牌</span><span class=\"mi\">39</span><span class=\"err\">枚，银牌</span><span class=\"mi\">41</span><span class=\"err\">枚，铜牌</span><span class=\"mi\">33</span><span class=\"err\">枚。所有奖牌数都超过了</span><span class=\"mi\">15</span><span class=\"err\">枚，因此美国符合条件。</span>\n\n<span class=\"mf\">2.</span> <span class=\"err\">中国（</span><span class=\"n\">CHN</span><span class=\"err\">）：金牌</span><span class=\"mi\">38</span><span class=\"err\">枚，银牌</span><span class=\"mi\">32</span><span class=\"err\">枚，铜牌</span><span class=\"mi\">19</span><span class=\"err\">枚。所有奖牌数都超过了</span><span class=\"mi\">15</span><span class=\"err\">枚，因此中国符合条件。</span>\n\n<span class=\"mf\">3.</span> <span class=\"err\">日本（</span><span class=\"n\">JPN</span><span class=\"err\">）：金牌</span><span class=\"mi\">27</span><span class=\"err\">枚，银牌</span><span class=\"mi\">14</span><span class=\"err\">枚，铜牌</span><span class=\"mi\">17</span><span class=\"err\">枚。只有金牌数超过了</span><span class=\"mi\">15</span><span class=\"err\">枚，因此日本不符合条件。</span>\n\n<span class=\"err\">因此，只有美国和中国符合条件。现在我们可以将这些信息绘制成表格：</span>\n\n<span class=\"o\">|</span> <span class=\"err\">排名</span> <span class=\"o\">|</span> <span class=\"err\">国家</span> <span class=\"o\">/</span> <span class=\"err\">地区</span>          <span class=\"o\">|</span> <span class=\"err\">金牌</span> <span class=\"o\">|</span> <span class=\"err\">银牌</span> <span class=\"o\">|</span> <span class=\"err\">铜牌</span> <span class=\"o\">|</span> <span class=\"err\">总计</span> <span class=\"o\">|</span>\n<span class=\"o\">|------|---------------------|------|------|------|------|</span>\n<span class=\"o\">|</span> <span class=\"mi\">1</span>    <span class=\"o\">|</span> <span class=\"err\">美国（</span><span class=\"n\">USA</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">39</span>   <span class=\"o\">|</span> <span class=\"mi\">41</span>   <span class=\"o\">|</span> <span class=\"mi\">33</span>   <span class=\"o\">|</span> <span class=\"mi\">113</span>  <span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mi\">2</span>    <span class=\"o\">|</span> <span class=\"err\">中国（</span><span class=\"n\">CHN</span><span class=\"err\">）</span>         <span class=\"o\">|</span> <span class=\"mi\">38</span>   <span class=\"o\">|</span> <span class=\"mi\">32</span>   <span class=\"o\">|</span> <span class=\"mi\">19</span>   <span class=\"o\">|</span> <span class=\"mi\">89</span>   <span class=\"o\">|</span>\n\n<span class=\"err\">请注意，日本（</span><span class=\"n\">JPN</span><span class=\"err\">）虽然金牌数超过了</span><span class=\"mi\">15</span><span class=\"err\">枚，但银牌和铜牌数没有达到要求，因此没有包含在表格中。</span></code></pre></div><h2>100万 Token 长文本支持，文档对话系统全开源</h2><p data-pid=\"Gs9cwYxd\">在长文档理解、复杂的智能体交互等应用场景中，模型的上下文长度支持有着更高的要求。InternLM2.5 提出了解决方案，将上下文长度从上一代模型 InternLM2 的 200K 提升到了 1M（约合 120 万汉字），进一步释放了模型在超长文本应用上的潜力。在模型的预训练中，我们从自然语料中筛选出了 256K Token 长度的文本，同时为了避免语料类型过于单一而导致的域偏移，我们通过合成数据进行了补充，使得模型在扩展上下文的同时可以尽量保留其能力。 </p><p data-pid=\"q6bXT60a\">我们采用了业界流行的“大海捞针”来评估模型的长文信息召回内容，下图显示，InternLM 2.5 在 1M token 范围内实现了几乎完美的大海捞针召回，呈现了极强的长文处理能力。</p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-c8dbb36471df3798fdaa60d39d7d94ca_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"790\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic1.zhimg.com/v2-c8dbb36471df3798fdaa60d39d7d94ca_r.jpg\" data-original-token=\"v2-c8dbb36471df3798fdaa60d39d7d94ca\"/></figure><p data-pid=\"KeMw2Nkb\">除此之外，我们还使用了广泛使用的长文理解能力评测集 LongBench 来进行评估，结果显示 InternLM2.5 取得了最优的性能。</p><table data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"><tbody><tr><td></td><td>GLM4-9B-Chat-1M</td><td>Qwen2-7B-Instruct</td><td>Yi1.5-9B-chat</td><td>InternLM2.5-7B-Chat-1M</td></tr><tr><td>Longbench</td><td>46.5</td><td>41.2</td><td>37.2</td><td>47.4</td></tr></tbody></table><p data-pid=\"P7D0UjU0\">依托于 InternLM2.5 的长文本能力，我们开发了文档对话应用，支持用户私有化部署模型，自由上传文档进行对话。并且整个系统全链路开源，方便用户一键搭建，包括 LMDeploy 长文本推理后端支持，MinerU 多类型文档的解析转换能力，基于 Streamlit 的前端对话体验工具等。目前支持 TXT，Markdown 和 PDF 文档，后续会持续支持如 Word、PPT 等多种办公文档类型。</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-a1ac33ee845d2e8197ad3728fca4b825_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1112\" data-rawheight=\"1280\" class=\"origin_image zh-lightbox-thumb\" width=\"1112\" data-original=\"https://pic4.zhimg.com/v2-a1ac33ee845d2e8197ad3728fca4b825_r.jpg\" data-original-token=\"v2-a1ac33ee845d2e8197ad3728fca4b825\"/></figure><h2>基于网络信息高效解决复杂问题</h2><p data-pid=\"c6iTc0Zr\">针对需要大规模复杂信息搜索和整合的复杂问题场景，InternLM2.5 创新性地提出了 MindSearch 多智能体框架，模拟人的思维过程，引入了任务规划、任务拆解、大规模网页搜索、多源信息归纳总结等步骤，有效地整合网络信息。其中，规划器专注于任务的规划、拆解和信息归纳，采用图结构编程的方式进行规划，并根据任务状态进行动态拓展，搜索器负责发散式搜索并总结网络搜索结果，使得整个框架能够基于上百个网页的信息进行筛选和浏览和整合。</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-1b21510ac5e09e463efc6919e28cc8e1_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"976\" data-rawheight=\"620\" class=\"origin_image zh-lightbox-thumb\" width=\"976\" data-original=\"https://picx.zhimg.com/v2-1b21510ac5e09e463efc6919e28cc8e1_r.jpg\" data-original-token=\"v2-1b21510ac5e09e463efc6919e28cc8e1\"/></figure><p data-pid=\"v2Z5pFGe\">经过针对性能力增强，InternLM2.5 能够有效从上百个网页中进行信息筛选、浏览和整合，解决专业的复杂问题，将人类需要 3 小时才能完成的调研总结工作缩短到了 3 分钟。如下面的视频所示，对于多步骤的复杂问题，模型能够分析用户需求，先搜索嫦娥 6 号的技术难点、再针对每一个技术难点搜索对应的解决方案，再从任务目标、技术手段、科学成果、国际合作 4 个方面对比阿波罗 11号 登月计划，最后总结我国探月成功的贡献。</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1792269118772228098\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://picx.zhimg.com/v2-ee86172b5538a0ceb5708e517038245c.jpg?source=382ee89a\" data-lens-id=\"1792269118772228098\"><img class=\"thumbnail\" src=\"https://picx.zhimg.com/v2-ee86172b5538a0ceb5708e517038245c.jpg?source=382ee89a\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1792269118772228098</span></span></a><h2>拥抱更广泛的开源生态</h2><p data-pid=\"Atc_k28U\">除了开源模型，书生·浦语从去年 7 月份开始推出了面向大模型研发与应用的全链条开源工具体系，贯穿数据、预训练、微调、部署、评测、应用六大环节。这些工具让用户能够更轻松地进行大模型的创新和应用，推动大模型开源生态的繁荣发展。随着 InternLM2.5 的发布，全链条工具体系也迎来了升级，对于应用环节进行了拓展，面向不同需求提供了新的工具，包括：</p><ul><li data-pid=\"Lrtgl70t\">HuixiangDou 领域知识助手（<a href=\"https://link.zhihu.com/?target=https%3A//github.com/InternLM/HuixiangDou\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/InternLM/Hui</span><span class=\"invisible\">xiangDou</span><span class=\"ellipsis\"></span></a>），专为处理群聊中的复杂技术问题而设计，适用于微信、飞书、钉钉等平台，提供完整的前后端 web、Android 及算法源码，支持工业级应用。</li><li data-pid=\"_FQpSEzw\">MinerU 智能数据提取工具（<a href=\"https://link.zhihu.com/?target=https%3A//github.com/opendatalab/MinerU\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/opendatalab/</span><span class=\"invisible\">MinerU</span><span class=\"ellipsis\"></span></a>），为多模态文档解析打造，不仅能将混合了图片、表格、公式等在内的多模态 PDF 文档精准转化为清晰、易于分析的 Markdown 格式，还能从包含广告等各种干扰信息的网页中快速解析、抽取正式内容</li></ul><p data-pid=\"3yj1DiRx\">除了自研的全链条开源工具体系之外，InternLM2.5 积极拥抱社区，兼容广泛的社区生态项目，主流开源项目“一网打尽”。</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-de7f75c4bc4e4ab82017f3c4dca8ffea_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic3.zhimg.com/v2-de7f75c4bc4e4ab82017f3c4dca8ffea_r.jpg\" data-original-token=\"v2-de7f75c4bc4e4ab82017f3c4dca8ffea\"/></figure><h2>书生大模型实战营</h2><p data-pid=\"K-_aQA6z\">上海人工智能实验室去年 12 月份推出了书生·浦语大模型实战营，收到社区的一致好评，半年来累计已有 15 万人次参与学习，并孵化出超 600 个生态项目。在 InternLM2.5 发布之际，我们也正式宣布书生·浦语大模型实战营正式升级为书生大模型实战营，逐步加入更多书生大模型体系课程与实战， 带你从入门到进阶，大模型时代不迷航。</p><p data-pid=\"fGUU9Q-n\">7 月 10 日至 8 月 10 日将正式开启第三期书生大模型实战营，在实战营中手把手带大家微调、部署 InternLM2.5 模型，免费算力及助教老师全程陪伴，还有权威的官方证书，快来<a href=\"https://link.zhihu.com/?target=https%3A//www.wjx.cn/vm/PvefmG2.aspx%3Fudsid%3D831608\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">报名</a>学习吧！</p><p data-pid=\"pTPXmcAh\">报名链接：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.wjx.cn/vm/PvefmG2.aspx%3Fudsid%3D831608\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">点击立即报名实战营</a><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-28b4ef6320c1f363d07f32233921c374_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2249\" data-rawheight=\"7909\" class=\"origin_image zh-lightbox-thumb\" width=\"2249\" data-original=\"https://pic1.zhimg.com/v2-28b4ef6320c1f363d07f32233921c374_r.jpg\" data-original-token=\"v2-28b4ef6320c1f363d07f32233921c374\"/></figure><h2>总结</h2><p data-pid=\"s6_jPQ3Q\">书生·浦语以持续的高质量开源赋能创新，坚持开源和免费商用，面向实际应用场景提供更好的模型和工具链。</p>",
        "excerpt": "2024 年 7 月 3 日，上海人工智能实验室与商汤科技联合香港中文大学和复旦大学正式发布新一代大语言模型书⽣·浦语2.5（InternLM2.5）。相比上一代模型，InternLM2.5 有三项突出亮点： 推理能力大幅提升，领先于国内外同量级开源模型，在部分维度上甚至超越十倍量级的 Llama3-70B；支持 1M tokens 上下文，能够处理百万字长文；具有强大的自主规划和工具调用能力，比如可以针对复杂问题，搜索上百个网页并进行整合分析。InternLM…",
        "excerpt_new": "2024 年 7 月 3 日，上海人工智能实验室与商汤科技联合香港中文大学和复旦大学正式发布新一代大语言模型书⽣·浦语2.5（InternLM2.5）。相比上一代模型，InternLM2.5 有三项突出亮点： 推理能力大幅提升，领先于国内外同量级开源模型，在部分维度上甚至超越十倍量级的 Llama3-70B；支持 1M tokens 上下文，能够处理百万字长文；具有强大的自主规划和工具调用能力，比如可以针对复杂问题，搜索上百个网页并进行整合分析。InternLM…",
        "preview_type": "default",
        "preview_text": "",
        "url": "https://zhuanlan.zhihu.com/p/707065312",
        "comment_permission": "all",
        "voteup_count": 17,
        "comment_count": 1,
        "image_url": "https://picx.zhimg.com/v2-246d7b3cf86d4cf3a9dda44b9f3e906c_r.jpg?source=172ae18b",
        "linkbox": {
          "url": "",
          "category": "",
          "pic": "",
          "title": ""
        },
        "reaction_instruction": {},
        "upvoted_followees": [],
        "voting": 0,
        "content_need_truncated": false,
        "force_login_when_click_read_more": false,
        "is_navigator": false,
        "navigator_vote": false,
        "vote_next_step": "vote"
      },
      "verb": "MEMBER_VOTEUP_ARTICLE",
      "created_time": 1720149082,
      "interaction": {
        "can_show_sticker": true
      },
      "actor": {
        "id": "ac113c7837148e5d43c18a06cf248d21",
        "name": "河畔草lxr",
        "headline": "",
        "type": "people",
        "user_type": "people",
        "url": "https://www.zhihu.com/people/liuxiaoran-34",
        "url_token": "liuxiaoran-34",
        "avatar_url": "https://pica.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_l.jpg?source=5a24d060&needBackground=1",
        "gender": 1,
        "is_following": false,
        "is_followed": false,
        "is_org": false,
        "badge": [],
        "badge_v2": {
          "title": "",
          "merged_badges": [],
          "detail_badges": [],
          "icon": "",
          "night_icon": ""
        },
        "vip_info": {
          "is_vip": false,
          "target_url": ""
        },
        "kvip_info": {
          "is_vip": false,
          "target_url": ""
        }
      },
      "action_text": "赞同了文章",
      "is_sticky": false
    },
    {
      "id": "1717413336442",
      "type": "feed",
      "target": {
        "id": "678876874",
        "type": "article",
        "author": {
          "id": "bfcfd15c45e6c4a3e1cfd94e032faa54",
          "name": "书生大模型",
          "headline": "https://chat.intern-ai.org.cn/",
          "type": "people",
          "user_type": "people",
          "url": "https://www.zhihu.com/people/InternLM",
          "url_token": "InternLM",
          "avatar_url": "https://picx.zhimg.com/v2-c132c17c7f65728bf8fb57e4a7493fd3_l.jpg?source=5a24d060&needBackground=1",
          "gender": 1,
          "is_following": false,
          "is_followed": false,
          "is_org": false,
          "badge": [],
          "badge_v2": {
            "title": "",
            "merged_badges": [],
            "detail_badges": [],
            "icon": "",
            "night_icon": ""
          },
          "vip_info": {
            "is_vip": false,
            "target_url": ""
          },
          "kvip_info": {
            "is_vip": false,
            "target_url": ""
          }
        },
        "created": 1705733113,
        "updated": 1705733113,
        "title": "【InternLM专栏】如何让 InternLM2 外推至 200K?",
        "excerpt_title": "",
        "content": "<p data-pid=\"Xzq-d4Yp\">本文主要介绍 InternLM2 的长文本能力使用，包括 InternLM2 的部署、InternLM2 外推示例、大海捞针评测及其效果。</p><h2>前言</h2><p data-pid=\"tvST3Wo0\">2023年是大语言模型突飞猛进的一年，也是长上下文领域突飞猛进的一年。一年前，很多NLP模型的输入上限还只是512个token；但如今，对于一个大语言模型，其最长支持上下文长度的输入动辄 100K token，我们的 InternLM2 也支持了 200K token 的输入与理解。</p><p data-pid=\"EtMP_J5H\">过去，我们能输入给模型的只会是一句话或者一段话，但长上下文意味着，我们可以喂给模型的是一本书，让它去做摘要问答；此外，我们过去只能和模型进行简单的几轮对话，但现在我们可以进行多轮对话，并且还可以要求模型在对话若干轮之后回忆之前很久远的内容（大海捞针）。</p><p data-pid=\"V5KEo0GI\">对于 InternLM2，我们使用</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.05209\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">旋转位置编码外推的缩放法则</a><p data-pid=\"Wn7gneK7\">及 Dynamic NTK 外推方法，在200K长文本上进行了大海捞针测试，以下是详细介绍。</p><h2>InternLM2 部署</h2><h3>推理框架 LMDeploy 安装</h3><p data-pid=\"6_lUt2nC\">在进行推理之前，我们需要准备包含 torch 和 LMDeploy 的环境。在长文本推理中，LMDeploy 会帮助我们大幅度节省推理中的显存占用。安装说明可以参考<a href=\"https://link.zhihu.com/?target=https%3A//github.com/InternLM/lmdeploy/blob/main/docs/zh_cn/build.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"> LMDeploy 的 github 仓库</a>。可直接使用 <code>pip install</code> 命令安装：</p><div class=\"highlight\"><pre><code class=\"language-bash\">pip install lmdeploy</code></pre></div><h3><br/>HF 模型和 LMDeploy 模型的转换</h3><p data-pid=\"vWiysuHB\">为了减少长文本推理时的显存占用，我们需要使用 <code>lmdeploy convert</code> 命令来将模型转换为 turbomind 格式。其中 <code>--tp</code> 参数的设置需要根据模型规模和显卡规模设置，可以参考 <b>推理参数推荐设置</b> 部分。</p><div class=\"highlight\"><pre><code class=\"language-bash\">lmdeploy convert internlm2-chat-7b internlm/internlm2-chat-7b --dst-path ./internlm2-chat-7b-turbomind --tp <span class=\"m\">1</span>\nlmdeploy convert internlm2-chat-20b internlm/internlm2-chat-20b --dst-path ./internlm2-chat-20b-turbomind --tp <span class=\"m\">2</span></code></pre></div><p data-pid=\"QmSh6SGX\">在上面的命令中，<code>lmdeploy</code>会首先下载<code>internlm/internlm2-chat-7b</code>或<code>internlm/internlm2-chat-20b</code>模型，并且进行格式转换。需要注意的一点是，<code>lmdeploy convert</code>需要 gpu，请确保在 cuda 环境下执行此命令。</p><h2>InternLM2 外推示例</h2><h3>推理参数推荐设置</h3><p data-pid=\"gzd9Oyz1\">在成功把 HuggingFace 格式的模型文件转换为 LMDeploy 使用的 turbomind 模型后，我们需打开对应路径下的<code>./internlm2-chat-7b-turbomind/triton_models/weights/config.ini</code> ，更改模型的一些推理设置。其中最重要的参数是以下三个：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">tensor_para_size</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">session_len</span> <span class=\"o\">=</span> <span class=\"mi\">210000</span>\n<span class=\"n\">rope_scaling_factor</span> <span class=\"o\">=</span> <span class=\"mf\">3.0</span></code></pre></div><p data-pid=\"i_N1ScUV\">其中<code>tensor_para_size</code>表示张量并行的参数、同时也是用几张 GPU 来运行一个模型。如果 GPU 的显存有80G，则<code>tensor_para_size=1</code>即可支撑 7B 模型 200K 上下文的推理，但如果 GPU 显存更小，可加大<code>tensor_para_size</code>来利用多个 GPU 实现 200K 推理。如果想要改变<code>tensor_para_size</code>，请重新使用上一小节的<code>lmdeploy convert</code>命令进行转换。下表列举了常见显卡大小对应的设置：</p><table data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"><tbody><tr><th>200K上下文推荐运行参数<br/>（tensor_para_size）</th><th>24G</th><th>32G</th><th>80G</th></tr><tr><td>InternLM2-Chat-7B</td><td>4</td><td>3</td><td>1</td></tr><tr><td>InternLM2-Chat-20B</td><td>8</td><td>6</td><td>2</td></tr></tbody></table><p data-pid=\"lv1Gj2KI\"><code>session_len</code>即模型最大的上下文窗口大小，为输入加输出总和的最大长度，推荐不小于 210K。</p><p data-pid=\"t-krIfLq\"><code>rope_scaling_factor</code>表示 Dynamic NTK 外推方法的系数，Dynamic NTK 相关算法可以参考<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.05209\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Scaling Laws of RoPE-based Extrapolation</a>。我们可以近似的认为、该值越高，模型在长文表现更好，但有损失短文表现的风险（ 32K 以下的文本不受影响，一般在过大时会导致 32K~64K 之间的语言质量下降）。由于 InternLM2 各版本的训练数据、流程都有差别，InternLM2 的 20B 模型的长文本训练数据量是7B 模型的三分之一左右，所以对应的经验最佳<code>rope_scaling_factor</code>设置也有区别，下表是几个推荐在长文本环境使用的版本及其推荐参数。</p><table data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"><tbody><tr><th>推荐参数</th><th>InternLM2-7B</th><th>InternLM2-Chat-7B-SFT</th><th>InternLM2-Chat-20B-SFT</th><th>InternLM2-Chat-7B</th><th>InternLM2-Chat-20B</th></tr><tr><td>rope_scaling_factor</td><td>5.0</td><td>2.0</td><td>3.0</td><td>2.0</td><td>3.0</td></tr></tbody></table><h3>200K inference 示例</h3><p data-pid=\"MVo1aZ0q\">在完成上面的配置之后，可参照下面的例子进行长文本推理：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># long_text.py</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lmdeploy</span> <span class=\"kn\">import</span> <span class=\"n\">pipeline</span><span class=\"p\">,</span> <span class=\"n\">GenerationConfig</span><span class=\"p\">,</span> <span class=\"n\">TurbomindEngineConfig</span>\n\n<span class=\"c1\"># backenc_config 会覆盖 ./internlm2-chat-7b/triton_models/weights/config.ini 中的配置</span>\n<span class=\"n\">backend_config</span> <span class=\"o\">=</span> <span class=\"n\">TurbomindEngineConfig</span><span class=\"p\">(</span><span class=\"n\">rope_scaling_factor</span><span class=\"o\">=</span><span class=\"mf\">2.0</span><span class=\"p\">,</span> <span class=\"n\">session_len</span><span class=\"o\">=</span><span class=\"mi\">210000</span><span class=\"p\">)</span>\n<span class=\"n\">pipe</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">(</span><span class=\"s1\">&#39;./internlm2-chat-7b-turbomind&#39;</span><span class=\"p\">,</span> <span class=\"n\">backend_config</span><span class=\"o\">=</span><span class=\"n\">backend_config</span><span class=\"p\">)</span>\n<span class=\"c1\"># prompt 可以替换为长文本的输入</span>\n<span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;你好&#39;</span>\n\n<span class=\"n\">gen_config</span> <span class=\"o\">=</span> <span class=\"n\">GenerationConfig</span><span class=\"p\">(</span><span class=\"n\">top_p</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span>\n                              <span class=\"n\">top_k</span><span class=\"o\">=</span><span class=\"mi\">40</span><span class=\"p\">,</span>\n                              <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span>\n                              <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">pipe</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"p\">,</span> <span class=\"n\">gen_config</span><span class=\"o\">=</span><span class=\"n\">gen_config</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n<span class=\"c1\"># Response(text=&#39;你好！很高兴能为你提供帮助。有什么我可以为您做的吗？&#39;, generate_token_len=15, finish_reason=&#39;stop&#39;)</span></code></pre></div><p data-pid=\"KUJX706G\">推荐使用 torchrun 命令启动：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"c1\"># --nproc_per_node 指定为 turbomind 模型的 tp 大小</span>\ntorchrun --nproc_per_node<span class=\"o\">=</span><span class=\"m\">1</span> long_text.py\n\n&gt; 输入：《钢铁是怎样炼成的》前30万字<span class=\"se\">\\n\\n</span>请为这本书写一段推荐评语\n&gt; InternLM2-Chat-7B：《钢铁是怎样炼成的》是一部描绘了俄国十月革命时期艰苦卓绝的斗争生活的史诗性小说。主人公保尔·柯察金在革命斗争中经历了种种磨难，包括被捕、被关押、伤残、失明等。然而，他始终保持了坚定的信念和坚强的意志，最终获得了胜利。这本书以朴实无华的笔触，描绘了苏维埃政权的诞生和布尔什维克党的崛起，展现了工人阶级为自由和幸福而斗争的历程。主人公保尔的形象，是一个勇于牺牲、不屈不挠的革命战士，他的精神品质和战斗精神鼓舞了无数青年投身革命事业。本书是社会主义文学的经典之作，值得一读。</code></pre></div><p data-pid=\"OBdBopC3\">需要注意的是 Chat 和 SFT 版本的模型需配套 <a href=\"https://link.zhihu.com/?target=https%3A//huggingface.co/internlm/internlm2-chat-7b/blob/main/modeling_internlm2.py%23L1160-L1162\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">meta_template</a> 使用才会有更好的体验。</p><h2>大海捞针评测</h2><h3>大海捞针任务介绍</h3><p data-pid=\"fJG8zYDW\">为了定量测试模型的长文本能力，我们采用OpenCompass的“大海捞针”来检测模型的性能，该试验通过将关键信息随机插入一段长文本的不同位置构造问题，测试模型是否能从长文本中提取出关键信息。比如下图中，我们把“小明最喜欢的地点”这一根“针”放到由多篇文档拼接成的超长文本中。每次测试投放一根针、投放位置会均匀覆盖整个文档。在输入的结尾，我们会提问模型“小明最喜欢的地方是哪里”，如果模型正确回答，则认为成功从“大海”里捞起了“针”，实现了长文中的关键信息召回。</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-f2e6df8e5d8389f9f3046e6f1dcc5b61_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"659\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://picx.zhimg.com/v2-f2e6df8e5d8389f9f3046e6f1dcc5b61_r.jpg\" data-original-token=\"v2-f2e6df8e5d8389f9f3046e6f1dcc5b61\"/><figcaption>“大海捞针”任务示例</figcaption></figure><h3>  大海捞针评测流程</h3><p data-pid=\"Apx-Bmg7\">OpenCompass大海捞针参考<br/></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/open-compass/opencompass\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OpenCompass</a><p data-pid=\"Q-0m5tn5\">  在使用时，请确保仓库包含了 commit id e975a96fa1995ab98ff2d6a4543876434af42e63 及之后的提交</p><div class=\"highlight\"><pre><code class=\"language-bash\">git clone https://github.com/open-compass/opencompass\n<span class=\"nb\">cd</span> opencompass\npip install -e .</code></pre></div><p data-pid=\"Mo4gGwFP\">然后下载大海捞针所需的数据集：</p><p data-pid=\"YtNiEnSn\">从 <a href=\"https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/Skywork/ChineseDomainModelingEval/tree/main\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ChineseDomainModelingEval</a> 下载数据集，放在 opencompass 的 <i>./data/CDME</i> 下：</p><div class=\"highlight\"><pre><code class=\"language-text\">opencompass/\n├── configs\n├── docs\n├── data\n│   └── CDME\n│       ├── processed\n│       ├── README.md\n│       ├── zh_finance.jsonl\n│       ├── zh_game.jsonl\n│       ├── zh_general.jsonl\n│       ├── zh_government.jsonl\n│       ├── zh_movie.jsonl\n│       └── zh_tech.jsonl\n├── LICENSE\n├── opencompass\n├── outputs\n├── run.py\n├── more...</code></pre></div><p data-pid=\"jO5Rw40l\">或者可以克隆相应的数据仓库：</p><div class=\"highlight\"><pre><code class=\"language-bash\">mkdir data <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">cd</span> data\ngit clone https://huggingface.co/datasets/Skywork/ChineseDomainModelingEval\nmv ChineseDomainModelingEval CDME</code></pre></div><p data-pid=\"zZoOh8Zw\">配置好数据集之后即可准备开始评测。评测 turbomind 模型大海捞针的文件为 <i><code>configs/eval_needleinahaystack_turbomind.py</code>：</i></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">opencompass.models.turbomind</span> <span class=\"kn\">import</span> <span class=\"n\">TurboMindModel</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">mmengine.config</span> <span class=\"kn\">import</span> <span class=\"n\">read_base</span>\n<span class=\"k\">with</span> <span class=\"n\">read_base</span><span class=\"p\">():</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">.datasets.cdme.cdme200k</span> <span class=\"kn\">import</span> <span class=\"n\">cdme_trim_datasets</span>\n\n<span class=\"n\">datasets</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"o\">*</span><span class=\"n\">cdme_trim_datasets</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># 旧版本 internlm2 chat 模板</span>\n<span class=\"c1\"># internlm2_meta_template = dict(</span>\n<span class=\"c1\">#     round=[</span>\n<span class=\"c1\">#         dict(role=&#39;HUMAN&#39;,</span>\n<span class=\"c1\">#              begin=&#39;[UNUSED_TOKEN_146]user\\n&#39;, end=&#39;[UNUSED_TOKEN_145]\\n&#39;),</span>\n<span class=\"c1\">#         dict(role=&#39;BOT&#39;, begin=&#39;[UNUSED_TOKEN_146]assistant\\n&#39;,</span>\n<span class=\"c1\">#              end=&#39;[UNUSED_TOKEN_145]\\n&#39;, generate=True),</span>\n<span class=\"c1\">#     ],</span>\n<span class=\"c1\"># )</span>\n<span class=\"c1\"># 新版本 internlm2 chat 模板</span>\n<span class=\"n\">internlm2_meta_template</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span>\n    <span class=\"nb\">round</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s1\">&#39;HUMAN&#39;</span><span class=\"p\">,</span>\n             <span class=\"n\">begin</span><span class=\"o\">=</span><span class=\"s1\">&#39;&lt;|im_start|&gt;user</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s1\">&#39;&lt;|im_end|&gt;</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span><span class=\"p\">),</span>\n        <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s1\">&#39;BOT&#39;</span><span class=\"p\">,</span> <span class=\"n\">begin</span><span class=\"o\">=</span><span class=\"s1\">&#39;&lt;|im_start|&gt;assistant</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span>\n             <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s1\">&#39;&lt;|im_end|&gt;</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span> <span class=\"n\">generate</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">),</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">models</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"c1\"># config for internlm2-chat-7b</span>\n    <span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"n\">TurboMindModel</span><span class=\"p\">,</span>\n        <span class=\"n\">abbr</span><span class=\"o\">=</span><span class=\"s1\">&#39;internlm2-chat-7b-turbomind&#39;</span><span class=\"p\">,</span>\n        <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">&#39;../internlm2-chat-7b-turbomind&#39;</span><span class=\"p\">,</span>\n        <span class=\"n\">max_out_len</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span>\n        <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n        <span class=\"n\">concurrency</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n        <span class=\"n\">meta_template</span><span class=\"o\">=</span><span class=\"n\">internlm2_meta_template</span><span class=\"p\">,</span>\n        <span class=\"n\">run_cfg</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">num_gpus</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">num_procs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n    <span class=\"p\">),</span>\n    <span class=\"c1\"># config for internlm2-20b</span>\n    <span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"n\">TurboMindModel</span><span class=\"p\">,</span>\n        <span class=\"n\">abbr</span><span class=\"o\">=</span><span class=\"s1\">&#39;internlm2-chat-20b-turbomind&#39;</span><span class=\"p\">,</span>\n        <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">&#39;../internlm2-chat-20b-turbomind&#39;</span><span class=\"p\">,</span>\n        <span class=\"n\">max_out_len</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span>\n        <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n        <span class=\"n\">concurrency</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n        <span class=\"n\">meta_template</span><span class=\"o\">=</span><span class=\"n\">internlm2_meta_template</span><span class=\"p\">,</span>\n        <span class=\"c1\"># 转换时 --tp 2，可以根据上文的 lmdeplot convert 命令中的 --tp 参数调整</span>\n        <span class=\"n\">run_cfg</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">num_gpus</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">num_procs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">]</span></code></pre></div><p data-pid=\"A4SqGPz7\">注：由于版本更迭，不同版本的 InternLM2 模型需要使用不同的 meta_template。可以用下面的命令进行简单地验证：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoTokenizer</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">&#34;/path/to/internlm2&#34;</span><span class=\"p\">,</span> <span class=\"n\">trust_remote_code</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">([</span><span class=\"mi\">92543</span><span class=\"p\">]))</span></code></pre></div><p data-pid=\"hbcNRLcG\">如果输出的是 [UNUSED_TOKEN_146]，则使用旧版本的 internlm2_meta_template；否则使用上文展示的对话模板。</p><p data-pid=\"kVIOFh_m\">然后在 opencompass 目录下运行命令：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">python</span> <span class=\"n\">run</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"n\">configs</span><span class=\"o\">/</span><span class=\"n\">eval_needleinahaystack_turbomind</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"o\">-</span><span class=\"n\">r</span></code></pre></div><p data-pid=\"36dRIp-i\">最终的生成结果、评测结果和结果表格均位于路径outputs/default/{datetime}下。</p><p data-pid=\"oVNc9bjc\">其中在进行生成时有可能会出现407 Proxy Authentication Required错误，这时可以重复执行上面的命令（注意添加 -r 参数）进行重试。</p><h3>大海捞针结果</h3><p data-pid=\"ZobWKD8u\">这里我们简单展示了 InternLM2-Chat-7B 和 InternLM2-Chat-20B 的结果，其中横轴为输入文本的总长，而纵轴表示关键信息“针”在文档中的位置，例如 0.0 表示文本开头，100.0 表示文本结尾。颜色代表了模型回答和标准答案之间的相对编辑距离。越接近绿色表明模型回答越准确。而平均深度分数是对某一长度的文本中所有文档位置测试的平均。平均深度分数在图中没有对应的坐标轴，越靠上，表明性能越好。</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-ab169453e766046f31ea34f78e926ee2_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-ab169453e766046f31ea34f78e926ee2_r.jpg\" data-original-token=\"v2-ab169453e766046f31ea34f78e926ee2\"/><figcaption>InternLM2-Chat-7B 200K上下文 大海捞针效果</figcaption></figure><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-deb8652c1c7bba3ef81d46c5f22caaf4_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-deb8652c1c7bba3ef81d46c5f22caaf4_r.jpg\" data-original-token=\"v2-deb8652c1c7bba3ef81d46c5f22caaf4\"/><figcaption>InternLM2-Chat-20B 200K上下文 大海捞针效果</figcaption></figure><p data-pid=\"6cf0mxcS\">   可以看到，上述两个模型的大海捞针测试结果很不错，而这得益于这一代模型针对长文本的训练方法、数据等方面的改进，这部分内容我们将择期分享。</p><p data-pid=\"1irUFAk-\">  【English Version Coming soon...】  </p><p data-pid=\"Hozqqy1p\">   觉得不错欢迎 star</p><p data-pid=\"n3H-zE6H\"><a href=\"https://link.zhihu.com/?target=https%3A//github.com/InternLM/InternLM\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/InternLM/Int</span><span class=\"invisible\">ernLM</span><span class=\"ellipsis\"></span></a> </p>",
        "excerpt": "本文主要介绍 InternLM2 的长文本能力使用，包括 InternLM2 的部署、InternLM2 外推示例、大海捞针评测及其效果。 前言2023年是大语言模型突飞猛进的一年，也是长上下文领域突飞猛进的一年。一年前，很多NLP模型的输入上限还只是512个token；但如今，对于一个大语言模型，其最长支持上下文长度的输入动辄 100K token，我们的 InternLM2 也支持了 200K token 的输入与理解。 过去，我们能输入给模型的只会是一句话或者一段话，但…",
        "excerpt_new": "本文主要介绍 InternLM2 的长文本能力使用，包括 InternLM2 的部署、InternLM2 外推示例、大海捞针评测及其效果。 前言2023年是大语言模型突飞猛进的一年，也是长上下文领域突飞猛进的一年。一年前，很多NLP模型的输入上限还只是512个token；但如今，对于一个大语言模型，其最长支持上下文长度的输入动辄 100K token，我们的 InternLM2 也支持了 200K token 的输入与理解。 过去，我们能输入给模型的只会是一句话或者一段话，但…",
        "preview_type": "default",
        "preview_text": "",
        "url": "https://zhuanlan.zhihu.com/p/678876874",
        "comment_permission": "all",
        "voteup_count": 9,
        "comment_count": 0,
        "image_url": "https://pica.zhimg.com/v2-1fa6b49ac7145ebbf1f9605b5b818cfd_r.jpg?source=172ae18b",
        "linkbox": {
          "url": "",
          "category": "",
          "pic": "",
          "title": ""
        },
        "reaction_instruction": {},
        "upvoted_followees": [],
        "voting": 0,
        "content_need_truncated": false,
        "force_login_when_click_read_more": false,
        "is_navigator": false,
        "navigator_vote": false,
        "vote_next_step": "vote"
      },
      "verb": "MEMBER_VOTEUP_ARTICLE",
      "created_time": 1717413336,
      "interaction": {
        "can_show_sticker": true
      },
      "actor": {
        "id": "ac113c7837148e5d43c18a06cf248d21",
        "name": "河畔草lxr",
        "headline": "",
        "type": "people",
        "user_type": "people",
        "url": "https://www.zhihu.com/people/liuxiaoran-34",
        "url_token": "liuxiaoran-34",
        "avatar_url": "https://pica.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_l.jpg?source=5a24d060&needBackground=1",
        "gender": 1,
        "is_following": false,
        "is_followed": false,
        "is_org": false,
        "badge": [],
        "badge_v2": {
          "title": "",
          "merged_badges": [],
          "detail_badges": [],
          "icon": "",
          "night_icon": ""
        },
        "vip_info": {
          "is_vip": false,
          "target_url": ""
        },
        "kvip_info": {
          "is_vip": false,
          "target_url": ""
        }
      },
      "action_text": "赞同了文章",
      "is_sticky": false
    },
    {
      "id": "1715689023541",
      "type": "feed",
      "target": {
        "id": "3498340410",
        "type": "answer",
        "url": "https://api.zhihu.com/answers/3498340410",
        "voteup_count": 66,
        "thanks_count": 1,
        "question": {
          "id": "655916303",
          "title": "如何评价 OpenAI 发布的支持实时语音对话的模型 GPT-4o?",
          "url": "https://api.zhihu.com/questions/655916303",
          "type": "question",
          "question_type": "normal",
          "created": 1715621070,
          "answer_count": 456,
          "comment_count": 27,
          "follower_count": 1658,
          "detail": "<p><a href=\"https://link.zhihu.com/?target=https%3A//openai.com/index/hello-gpt-4o/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">openai.com/index/hello-</span><span class=\"invisible\">gpt-4o/</span><span class=\"ellipsis\"></span></a></p>",
          "excerpt": "<a href=\"https://link.zhihu.com/?target=https%3A//openai.com/index/hello-gpt-4o/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">openai.com/index/hello-</span><span class=\"invisible\">gpt-4o/</span><span class=\"ellipsis\"></span></a>",
          "bound_topic_ids": [
            350,
            2143,
            180476,
            1963575,
            2431446
          ],
          "relationship": {
            "is_author": false
          },
          "is_following": false,
          "author": {
            "id": "c70d26c78493615bb39a6801e1e4ebe6",
            "name": "居居",
            "headline": "整点科技选题",
            "type": "people",
            "user_type": "people",
            "url": "https://www.zhihu.com/people/oinkoinkoink",
            "url_token": "oinkoinkoink",
            "avatar_url": "https://pic1.zhimg.com/v2-434a2d42f8c1524d5be33022a3ba8296_l.jpg?source=5a24d060&needBackground=1",
            "gender": 0,
            "is_following": false,
            "is_followed": false,
            "is_org": false,
            "badge": [],
            "badge_v2": {
              "title": "",
              "merged_badges": [],
              "detail_badges": [],
              "icon": "",
              "night_icon": ""
            },
            "vip_info": {
              "is_vip": false,
              "target_url": ""
            },
            "kvip_info": {
              "is_vip": false,
              "target_url": ""
            }
          },
          "reaction_instruction": null
        },
        "updated_time": 1715741858,
        "created_time": 1715685774,
        "author": {
          "id": "b19ca4e637b539cb250328d41bb3bb5b",
          "name": "邱锡鹏",
          "headline": "自然语言处理、机器学习、深度学习",
          "type": "people",
          "user_type": "people",
          "url": "https://www.zhihu.com/people/xpqiu",
          "url_token": "xpqiu",
          "avatar_url": "https://picx.zhimg.com/1590fdda3cae8d6c7a01ef595d0258e6_l.jpg?source=5a24d060&needBackground=1",
          "gender": 1,
          "is_following": true,
          "is_followed": false,
          "is_org": false,
          "badge": [
            {
              "type": "identity",
              "description": "复旦大学 计算机科学技术学院教授",
              "topic_names": [],
              "topics": []
            }
          ],
          "badge_v2": {
            "title": "复旦大学 计算机科学技术学院教授",
            "merged_badges": [
              {
                "type": "identity",
                "detail_type": "identity_people",
                "title": "认证",
                "description": "复旦大学 计算机科学技术学院教授",
                "url": "https://zhuanlan.zhihu.com/p/96956163",
                "sources": [],
                "icon": "",
                "night_icon": ""
              }
            ],
            "detail_badges": [
              {
                "type": "identity",
                "detail_type": "identity_people",
                "title": "已认证的个人",
                "description": "复旦大学 计算机科学技术学院教授",
                "url": "https://zhuanlan.zhihu.com/p/96956163",
                "sources": [],
                "icon": "https://pic1.zhimg.com/v2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c",
                "night_icon": "https://picx.zhimg.com/v2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c"
              }
            ],
            "icon": "https://picx.zhimg.com/v2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c",
            "night_icon": "https://picx.zhimg.com/v2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c"
          },
          "vip_info": {
            "is_vip": false,
            "target_url": ""
          },
          "kvip_info": {
            "is_vip": true,
            "vip_icon": {
              "url": "https://picx.zhimg.com/v2-57fe7feb4813331d5eca02ef731e12c9_r.jpg?source=5a24d060",
              "night_mode_url": "https://picx.zhimg.com/v2-57fe7feb4813331d5eca02ef731e12c9_r.jpg?source=5a24d060"
            },
            "target_url": "https://www.zhihu.com/kvip/purchase"
          }
        },
        "comment_permission": "all",
        "is_copyable": true,
        "comment_count": 3,
        "can_comment": {
          "status": false,
          "reason": "只有作者关注的人才可以评论"
        },
        "content": "<p data-pid=\"ch_7QA2e\">搭着GPT4o的风头，介绍下我们去年的两个工作：</p><p data-pid=\"ZX7eNwQV\">1）SpeechGPT和SpeechTokenizer：让大模型直接听懂和产生语音   </p><figure data-size=\"normal\"><noscript><img src=\"https://picx.zhimg.com/v2-4bba56f52186447103d090591b6c1445_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"759\" data-original-token=\"v2-b9d647aa4e9a624dca7b3d987caf8414\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-975169e4649a8b7f7e5a68431b5bc282_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1694\" data-original=\"https://picx.zhimg.com/v2-4bba56f52186447103d090591b6c1445_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1694&#39; height=&#39;759&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"759\" data-original-token=\"v2-b9d647aa4e9a624dca7b3d987caf8414\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-975169e4649a8b7f7e5a68431b5bc282_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1694\" data-original=\"https://picx.zhimg.com/v2-4bba56f52186447103d090591b6c1445_r.jpg\" data-actualsrc=\"https://picx.zhimg.com/v2-4bba56f52186447103d090591b6c1445_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=https%3A//github.com/0nutation/SpeechGPT\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - 0nutation/SpeechGPT: SpeechGPT Series: Speech Large Language Models</a><p data-pid=\"VVeneEXo\">2）AnyGPT：基于离散序列建模的统一多模态模型  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fbc296d29cbab121559ed44d87b15c72_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1358\" data-rawheight=\"765\" data-original-token=\"v2-559e1bb242b2543aaf3dcc7564d01694\" data-default-watermark-src=\"https://picx.zhimg.com/v2-9dd7169baeb5286195db18be302b185d_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1358\" data-original=\"https://pic1.zhimg.com/v2-fbc296d29cbab121559ed44d87b15c72_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1358&#39; height=&#39;765&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1358\" data-rawheight=\"765\" data-original-token=\"v2-559e1bb242b2543aaf3dcc7564d01694\" data-default-watermark-src=\"https://picx.zhimg.com/v2-9dd7169baeb5286195db18be302b185d_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1358\" data-original=\"https://pic1.zhimg.com/v2-fbc296d29cbab121559ed44d87b15c72_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fbc296d29cbab121559ed44d87b15c72_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=https%3A//github.com/OpenMOSS/AnyGPT\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - OpenMOSS/AnyGPT: Code for &#34;AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling&#34;</a><p></p>",
        "excerpt": "搭着GPT4o的风头，介绍下我们去年的两个工作： 1）SpeechGPT和SpeechTokenizer：让大模型直接听懂和产生语音 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/0nutation/SpeechGPT\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - 0nutation/SpeechGPT: SpeechGPT Series: Speech Large Language Models</a>2）AnyGPT：基于离散序列建模的统一多模态模型 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/OpenMOSS/AnyGPT\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - OpenMOSS/AnyGPT: Code for &#34;AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling&#34;</a> ",
        "excerpt_new": "搭着GPT4o的风头，介绍下我们去年的两个工作： 1）SpeechGPT和SpeechTokenizer：让大模型直接听懂和产生语音 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/0nutation/SpeechGPT\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - 0nutation/SpeechGPT: SpeechGPT Series: Speech Large Language Models</a>2）AnyGPT：基于离散序列建模的统一多模态模型 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/OpenMOSS/AnyGPT\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - OpenMOSS/AnyGPT: Code for &#34;AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling&#34;</a> ",
        "preview_type": "default",
        "preview_text": "",
        "relationship": {
          "voting": 0,
          "is_thanked": false,
          "is_nothelp": false,
          "upvoted_followee_ids": null
        },
        "reaction_instruction": {},
        "relevant_info": {
          "relevant_type": "",
          "is_relevant": false,
          "relevant_text": ""
        },
        "reshipment_settings": "allowed",
        "answer_type": "normal",
        "paid_info": {
          "type": "",
          "content": "",
          "has_purchased": false
        },
        "attachment": null,
        "is_navigator": false,
        "navigator_vote": false,
        "vote_next_step": "vote",
        "mark_infos": [],
        "content_need_truncated": false,
        "force_login_when_click_read_more": false
      },
      "verb": "MEMBER_VOTEUP_ANSWER",
      "created_time": 1715689023,
      "interaction": {
        "can_show_sticker": true
      },
      "actor": {
        "id": "ac113c7837148e5d43c18a06cf248d21",
        "name": "河畔草lxr",
        "headline": "",
        "type": "people",
        "user_type": "people",
        "url": "https://www.zhihu.com/people/liuxiaoran-34",
        "url_token": "liuxiaoran-34",
        "avatar_url": "https://picx.zhimg.com/v2-58065fdefaf543f86cccc363923d8444_l.jpg?source=5a24d060&needBackground=1",
        "gender": 1,
        "is_following": false,
        "is_followed": false,
        "is_org": false,
        "badge": [],
        "badge_v2": {
          "title": "",
          "merged_badges": [],
          "detail_badges": [],
          "icon": "",
          "night_icon": ""
        },
        "vip_info": {
          "is_vip": false,
          "target_url": ""
        },
        "kvip_info": {
          "is_vip": false,
          "target_url": ""
        }
      },
      "action_text": "赞同了回答",
      "is_sticky": false
    }
  ],
  "paging": {
    "is_end": false,
    "need_force_login": false,
    "next": "https://www.zhihu.com/api/v3/moments/liuxiaoran-34/activities?offset=1715689023541&page_num=1"
  }
}